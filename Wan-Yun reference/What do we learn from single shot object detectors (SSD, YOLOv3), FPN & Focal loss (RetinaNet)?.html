<!DOCTYPE html><html xmlns:cc="http://creativecommons.org/ns#"><head prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# medium-com: http://ogp.me/ns/fb/medium-com#"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, viewport-fit=contain"><title>What do we learn from single shot object detectors (SSD, YOLOv3), FPN &amp; Focal loss (RetinaNet)?</title><link rel="canonical" href="https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d"><meta name="title" content="What do we learn from single shot object detectors (SSD, YOLOv3), FPN &amp; Focal loss (RetinaNet)?"><meta name="referrer" content="unsafe-url"><meta name="description" content="In part 2, we will have a comprehensive review of single shot object detectors including SSD and YOLO (YOLOv2 and YOLOv3). We will also look into FPN to see how a pyramid of multi-scale feature maps…"><meta name="theme-color" content="#000000"><meta property="og:title" content="What do we learn from single shot object detectors (SSD, YOLOv3), FPN &amp; Focal loss (RetinaNet)?"><meta property="twitter:title" content="What do we learn from single shot object detectors (SSD, YOLOv3), FPN &amp; Focal loss (RetinaNet)?"><meta property="og:url" content="https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d"><meta property="og:image" content="https://cdn-images-1.medium.com/max/1200/0*OG_cQNTRdw0CYGyU."><meta property="fb:app_id" content="542599432471018"><meta property="og:description" content="In part 2, we will have a comprehensive review of single shot object detectors including SSD and YOLO (YOLOv2 and YOLOv3). We will also…"><meta name="twitter:description" content="In part 2, we will have a comprehensive review of single shot object detectors including SSD and YOLO (YOLOv2 and YOLOv3). We will also…"><meta name="twitter:image:src" content="https://cdn-images-1.medium.com/max/1200/0*OG_cQNTRdw0CYGyU."><link rel="author" href="https://medium.com/@jonathan_hui"><meta name="author" content="Jonathan Hui"><meta property="og:type" content="article"><meta name="twitter:card" content="summary_large_image"><meta property="article:publisher" content="https://www.facebook.com/medium"><meta property="article:author" content="https://medium.com/@jonathan_hui"><meta name="robots" content="index, follow"><meta property="article:published_time" content="2018-03-28T06:13:37.724Z"><meta name="twitter:site" content="@Medium"><meta property="og:site_name" content="Medium"><meta name="twitter:label1" value="Reading time"><meta name="twitter:data1" value="9 min read"><meta name="twitter:app:name:iphone" content="Medium"><meta name="twitter:app:id:iphone" content="828256236"><meta name="twitter:app:url:iphone" content="medium://p/3888677c5f4d"><meta property="al:ios:app_name" content="Medium"><meta property="al:ios:app_store_id" content="828256236"><meta property="al:android:package" content="com.medium.reader"><meta property="al:android:app_name" content="Medium"><meta property="al:ios:url" content="medium://p/3888677c5f4d"><meta property="al:android:url" content="medium://p/3888677c5f4d"><meta property="al:web:url" content="https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d"><link rel="search" type="application/opensearchdescription+xml" title="Medium" href="/osd.xml" /><link rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/3888677c5f4d" /><script type="application/ld+json">{"@context":"http://schema.org","@type":"NewsArticle","image":{"@type":"ImageObject","width":1920,"height":1110,"url":"https://cdn-images-1.medium.com/max/2600/0*OG_cQNTRdw0CYGyU."},"url":"https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d","dateCreated":"2018-03-28T06:13:37.724Z","datePublished":"2018-03-28T06:13:37.724Z","dateModified":"2019-04-15T17:30:09.430Z","headline":"What do we learn from single shot object detectors (SSD, YOLOv3), FPN & Focal loss (RetinaNet)?","name":"What do we learn from single shot object detectors (SSD, YOLOv3), FPN & Focal loss (RetinaNet)?","articleId":"3888677c5f4d","thumbnailUrl":"https://cdn-images-1.medium.com/max/2600/0*OG_cQNTRdw0CYGyU.","keywords":["Tag:Machine Learning","Tag:Deep Learning","Tag:Computer Vision","Tag:Object Detection","Tag:Artificial Intelligence","LockedPostSource:0","Elevated:false","LayerCake:0"],"author":{"@type":"Person","name":"Jonathan Hui","url":"https://medium.com/@jonathan_hui"},"creator":["Jonathan Hui"],"publisher":{"@type":"Organization","name":"Medium","url":"https://medium.com/","logo":{"@type":"ImageObject","width":308,"height":60,"url":"https://cdn-images-1.medium.com/max/616/1*OMF3fSqH8t4xBJ9-6oZDZw.png"}},"mainEntityOfPage":"https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d"}</script><meta name="parsely-link" content="https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d"><link rel="stylesheet" type="text/css" class="js-glyph-" id="glyph-8" href="https://glyph.medium.com/css/e/sr/latin/e/ssr/latin/e/ssb/latin/m2.css" /><link rel="stylesheet" href="https://cdn-static-1.medium.com/_/fp/css/main-branding-base.EatMsbQpPoYCFsH18s5T4g.css"><script>!function(n,e){var t,o,i,c=[],f={passive:!0,capture:!0},r=new Date,a="pointerup",u="pointercancel";function p(n,c){t||(t=c,o=n,i=new Date,w(e),s())}function s(){o>=0&&o<i-r&&(c.forEach(function(n){n(o,t)}),c=[])}function l(t){if(t.cancelable){var o=(t.timeStamp>1e12?new Date:performance.now())-t.timeStamp;"pointerdown"==t.type?function(t,o){function i(){p(t,o),r()}function c(){r()}function r(){e(a,i,f),e(u,c,f)}n(a,i,f),n(u,c,f)}(o,t):p(o,t)}}function w(n){["click","mousedown","keydown","touchstart","pointerdown"].forEach(function(e){n(e,l,f)})}w(n),self.perfMetrics=self.perfMetrics||{},self.perfMetrics.onFirstInputDelay=function(n){c.push(n),s()}}(addEventListener,removeEventListener);</script><script>if (window.top !== window.self) window.top.location = window.self.location.href;var OB_startTime = new Date().getTime(); var OB_loadErrors = []; function _onerror(e) { OB_loadErrors.push(e) }; if (document.addEventListener) document.addEventListener("error", _onerror, true); else if (document.attachEvent) document.attachEvent("onerror", _onerror); function _asyncScript(u) {var d = document, f = d.getElementsByTagName("script")[0], s = d.createElement("script"); s.type = "text/javascript"; s.async = true; s.src = u; f.parentNode.insertBefore(s, f);}function _asyncStyles(u) {var d = document, f = d.getElementsByTagName("script")[0], s = d.createElement("link"); s.rel = "stylesheet"; s.href = u; f.parentNode.insertBefore(s, f); return s}(new Image()).src = "/_/stat?event=pixel.load&origin=" + encodeURIComponent(location.origin);</script><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date; ga("create", "UA-24232453-2", "auto", {"allowLinker": true, "legacyCookieDomain": window.location.hostname}); ga("send", "pageview");</script><script async src="https://www.google-analytics.com/analytics.js"></script><!--[if lt IE 9]><script charset="UTF-8" src="https://cdn-static-1.medium.com/_/fp/js/shiv.RI2ePTZ5gFmMgLzG5bEVAA.js"></script><![endif]--><link rel="icon" href="https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium.3Y6xpZ-0FSdWDnPM3hSBIA.ico" class="js-favicon"><link rel="apple-touch-icon" sizes="152x152" href="https://cdn-images-1.medium.com/fit/c/304/304/1*8I-HPL0bfoIzGied-dzOvA.png"><link rel="apple-touch-icon" sizes="120x120" href="https://cdn-images-1.medium.com/fit/c/240/240/1*8I-HPL0bfoIzGied-dzOvA.png"><link rel="apple-touch-icon" sizes="76x76" href="https://cdn-images-1.medium.com/fit/c/152/152/1*8I-HPL0bfoIzGied-dzOvA.png"><link rel="apple-touch-icon" sizes="60x60" href="https://cdn-images-1.medium.com/fit/c/120/120/1*8I-HPL0bfoIzGied-dzOvA.png"><link rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg" color="#171717"></head><body itemscope class=" postShowScreen browser-safari os-mac is-withMagicUnderlines v-glyph v-glyph--m2 is-noJs"><script>document.body.className = document.body.className.replace(/(^|\s)is-noJs(\s|$)/, "$1is-js$2")</script><div class="site-main" id="container"><div class="butterBar butterBar--error"></div><div class="surface"><div id="prerendered" class="screenContent"><canvas class="canvas-renderer"></canvas><div class="container u-maxWidth740 u-xs-margin0 notesPositionContainer js-notesPositionContainer"></div><div class="metabar u-clearfix u-boxShadow4px12pxBlackLightest u-fixed u-backgroundTransparentWhiteDarkest u-xs-sizeFullViewportWidth js-metabar"><div class="branch-journeys-top"></div><div class="js-metabarMiddle metabar-inner u-marginAuto u-maxWidth1032 u-flexCenter u-justifyContentSpaceBetween u-height65 u-xs-height56 u-paddingHorizontal20"><div class="metabar-block u-flex1 u-flexCenter"><div class="u-xs-hide js-metabarLogoLeft"><a href="https://medium.com/" data-log-event="home" class="siteNav-logo u-fillTransparentBlackDarker u-flex0 u-flexCenter u-paddingTop0"><span class="svgIcon svgIcon--logoMonogram svgIcon--45px is-flushLeft u-flex0 u-flexCenter u-paddingTop0"><svg class="svgIcon-use" width="45" height="45" ><path d="M5 40V5h35v35H5zm8.56-12.627c0 .555-.027.687-.318 1.03l-2.457 2.985v.396h6.974v-.396l-2.456-2.985c-.291-.343-.344-.502-.344-1.03V18.42l6.127 13.364h.714l5.256-13.364v10.644c0 .29 0 .342-.185.528l-1.848 1.796v.396h9.19v-.396l-1.822-1.796c-.184-.186-.21-.238-.21-.528V15.937c0-.291.026-.344.21-.528l1.823-1.797v-.396h-6.471l-4.622 11.542-5.203-11.542h-6.79v.396l2.14 2.64c.239.292.291.37.291.768v10.353z"/></svg></span><span class="u-textScreenReader">Homepage</span></a></div><div class="u-xs-show js-metabarLogoLeft"><a href="https://medium.com/" data-log-event="home" class="siteNav-logo u-fillTransparentBlackDarker u-flex0 u-flexCenter u-paddingTop0"><span class="svgIcon svgIcon--logoMonogram svgIcon--45px is-flushLeft u-flex0 u-flexCenter u-paddingTop0"><svg class="svgIcon-use" width="45" height="45" ><path d="M5 40V5h35v35H5zm8.56-12.627c0 .555-.027.687-.318 1.03l-2.457 2.985v.396h6.974v-.396l-2.456-2.985c-.291-.343-.344-.502-.344-1.03V18.42l6.127 13.364h.714l5.256-13.364v10.644c0 .29 0 .342-.185.528l-1.848 1.796v.396h9.19v-.396l-1.822-1.796c-.184-.186-.21-.238-.21-.528V15.937c0-.291.026-.344.21-.528l1.823-1.797v-.396h-6.471l-4.622 11.542-5.203-11.542h-6.79v.396l2.14 2.64c.239.292.291.37.291.768v10.353z"/></svg></span><span class="u-textScreenReader">Homepage</span></a></div></div><div class="metabar-block u-flex0"><div class="buttonSet buttonSet--wide"><a class="button button--chromeless u-baseColor--buttonNormal u-xs-hide js-upgradeMembershipAction"   href="https://medium.com/membership?source=upgrade_membership---nav_full">Become a member</a><a class="button button--primary button--chromeless u-accentColor--buttonNormal is-inSiteNavBar u-xs-hide js-signInButton"   href="https://medium.com/m/signin?redirect=https%3A%2F%2Fmedium.com%2F%40jonathan_hui%2Fwhat-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d&amp;source=--------------------------nav_reg&amp;operation=login" data-action="sign-in-prompt" data-redirect="https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d" data-action-source="--------------------------nav_reg">Sign in</a><a class="button button--primary button--withChrome u-accentColor--buttonNormal is-inSiteNavBar js-signUpButton"   href="https://medium.com/m/signin?redirect=https%3A%2F%2Fmedium.com%2F%40jonathan_hui%2Fwhat-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d&amp;source=--------------------------nav_reg&amp;operation=register" data-action="sign-up-prompt" data-redirect="https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d" data-action-source="--------------------------nav_reg">Get started</a></div></div></div></div><div class="metabar metabar--spacer js-metabarSpacer u-height65 u-xs-height56"></div><main role="main"><article class=" u-minHeight100vhOffset65 u-overflowHidden postArticle postArticle--full is-withLeadingImage u-marginBottom40"  lang="en"><div class="postArticle-content js-postField js-notesSource js-trackPostScrolls"  data-post-id="3888677c5f4d" data-source="post_page" data-tracking-context="postPage"><section name="b530" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--outsetColumn"><figure name="b699" id="b699" class="graf graf--figure graf--layoutOutsetCenter graf--leading"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1032px; max-height: 597px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 57.8%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="0*OG_cQNTRdw0CYGyU." data-width="3000" data-height="1735" data-is-featured="true" data-action="zoom" data-action-value="0*OG_cQNTRdw0CYGyU."><img src="https://cdn-images-1.medium.com/freeze/max/60/0*OG_cQNTRdw0CYGyU.?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/2400/0*OG_cQNTRdw0CYGyU."><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/2400/0*OG_cQNTRdw0CYGyU."></noscript></div></div></figure></div><div class="section-inner sectionLayout--insetColumn"><h1 name="4928" id="4928" class="graf graf--h3 graf-after--figure graf--title">What do we learn from single shot object detectors (SSD, YOLOv3), FPN &amp; Focal loss (RetinaNet)?</h1><div class="uiScale uiScale-ui--regular uiScale-caption--regular u-flexCenter u-marginVertical24 u-fontSize15 js-postMetaLockup"><div class="u-flex0"><a class="link u-baseColor--link avatar"   href="https://medium.com/@jonathan_hui?source=post_header_lockup" data-action="show-user-card" data-action-source="post_header_lockup" data-action-value="bd51f1a63813" data-action-type="hover" data-user-id="bd51f1a63813" dir="auto"><img  src="https://cdn-images-1.medium.com/fit/c/100/100/1*c3Z3aOPBooxEX4tx4RkzLw.jpeg" class="avatar-image u-size50x50" alt="Go to the profile of Jonathan Hui"></a></div><div class="u-flex1 u-paddingLeft15 u-overflowHidden"><div class="u-paddingBottom3"><a class="ds-link ds-link--styleSubtle ui-captionStrong u-inlineBlock link link--darken link--darker"   href="https://medium.com/@jonathan_hui" data-action="show-user-card" data-action-value="bd51f1a63813" data-action-type="hover" data-user-id="bd51f1a63813" dir="auto">Jonathan Hui</a><span class="followState js-followState" data-user-id="bd51f1a63813"><button class="button button--smallest u-noUserSelect button--withChrome u-baseColor--buttonNormal button--withHover button--unblock js-unblockButton u-marginLeft10 u-xs-hide"  data-action="sign-up-prompt" data-sign-in-action="toggle-block-user" data-requires-token="true" data-redirect="https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d" data-action-source="post_header_lockup"><span class="button-label  button-defaultState">Blocked</span><span class="button-label button-hoverState">Unblock</span></button><button class="button button--primary button--smallest button--dark u-noUserSelect button--withChrome u-accentColor--buttonDark button--follow js-followButton u-marginLeft10 u-xs-hide"  data-action="sign-up-prompt" data-sign-in-action="toggle-subscribe-user" data-requires-token="true" data-redirect="https://medium.com/_/subscribe/user/bd51f1a63813" data-action-source="post_header_lockup-bd51f1a63813-------------------------follow_byline"><span class="button-label  button-defaultState js-buttonLabel">Follow</span><span class="button-label button-activeState">Following</span></button></span></div><div class="ui-caption u-noWrapWithEllipsis js-testPostMetaInlineSupplemental"><time datetime="2018-03-28T06:13:37.724Z">Mar 28, 2018</time><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="9 min read"></span></div></div></div><p name="b8c0" id="b8c0" class="graf graf--p graf-after--h3">In part 2, we will have a comprehensive review of single shot object detectors including SSD and YOLO (YOLOv2 and YOLOv3). We will also look into FPN to see how a pyramid of multi-scale feature maps will improve accuracy, in particular for small objects that usually perform badly for single shot detectors. Then we will look into Focal loss and RetinaNet on how it solve class imbalance problem during training.</p><p name="5461" id="5461" class="graf graf--p graf-after--p"><a href="https://medium.com/@jonathan_hui/what-do-we-learn-from-region-based-object-detectors-faster-r-cnn-r-fcn-fpn-7e354377a7c9" data-href="https://medium.com/@jonathan_hui/what-do-we-learn-from-region-based-object-detectors-faster-r-cnn-r-fcn-fpn-7e354377a7c9" class="markup--anchor markup--p-anchor" target="_blank">Part 1</a>: What do we learn from region based object detectors (Faster R-CNN, R-FCN, FPN)?</p><p name="b144" id="b144" class="graf graf--p graf-after--p"><a href="https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d" data-href="https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d" class="markup--anchor markup--p-anchor" target="_blank">Part 2</a>: What do we learn from single shot object detectors (SSD, YOLO), FPN &amp; Focal loss?</p><p name="f030" id="f030" class="graf graf--p graf-after--p"><a href="https://medium.com/@jonathan_hui/design-choices-lessons-learned-and-trends-for-object-detections-4f48b59ec5ff" data-href="https://medium.com/@jonathan_hui/design-choices-lessons-learned-and-trends-for-object-detections-4f48b59ec5ff" class="markup--anchor markup--p-anchor" target="_blank">Part 3</a>: Design choices, lessons learned and trends for object detections?</p><h3 name="5871" id="5871" class="graf graf--h3 graf-after--p">Single Shot detectors</h3><p name="55e6" id="55e6" class="graf graf--p graf-after--h3">Faster R-CNN has a dedicated region proposal network followed by a classifier.</p><figure name="1de8" id="1de8" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 215px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 30.7%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*F-WbcUMpWSE1tdKRgew2Ug.png" data-width="1455" data-height="447" data-action="zoom" data-action-value="1*F-WbcUMpWSE1tdKRgew2Ug.png"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*F-WbcUMpWSE1tdKRgew2Ug.png?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*F-WbcUMpWSE1tdKRgew2Ug.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*F-WbcUMpWSE1tdKRgew2Ug.png"></noscript></div></div><figcaption class="imageCaption">Faster R-CNN flow</figcaption></figure><p name="43b5" id="43b5" class="graf graf--p graf-after--figure">Region-based detectors are accurate but not without a cost. Faster R-CNN processes about 7 <strong class="markup--strong markup--p-strong">FPS</strong> (frame per second) for PASCAL VOC 2007 testing set. Like R-FCN, researchers are streamlining the process by reducing the amount of work for each ROI.</p><pre name="1627" id="1627" class="graf graf--pre graf-after--p">feature_maps = process(image)<br>ROIs = region_proposal(feature_maps)<br>for ROI in ROIs<br>    patch = roi_align(feature_maps, ROI)<br>    results = detector2(patch)    # Reduce the amount of work here!</pre><p name="a84d" id="a84d" class="graf graf--p graf-after--pre">As an alternative, do we need a separate region proposal step? Can we derive both boundary boxes and classes directly from feature maps in one step?</p><pre name="a55f" id="a55f" class="graf graf--pre graf-after--p">feature_maps = process(image)<br>results = detector3(feature_maps) # No more separate step for ROIs</pre><p name="7cfb" id="7cfb" class="graf graf--p graf-after--pre">Let’s look at the sliding-window detector again. We can slide windows over feature maps to detect objects. For different object types, we use different window shapes. The fatal mistake of the previous sliding-windows is that we use the windows as the final boundary boxes. For that, we need too many shapes to cover most objects. A more effective solution is to treat the window as an initial guess. Then we have a detector to predict the class and the boundary box from the current sliding window simultaneously.</p></div><div class="section-inner sectionLayout--outsetColumn"><figure name="ccf2" id="ccf2" class="graf graf--figure graf--layoutOutsetCenter graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1032px; max-height: 516px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 50%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*tE6DUwv6VIHu1KlwYmSBTw.jpeg" data-width="1123" data-height="561" data-action="zoom" data-action-value="1*tE6DUwv6VIHu1KlwYmSBTw.jpeg"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*tE6DUwv6VIHu1KlwYmSBTw.jpeg?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/2400/1*tE6DUwv6VIHu1KlwYmSBTw.jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/2400/1*tE6DUwv6VIHu1KlwYmSBTw.jpeg"></noscript></div></div><figcaption class="imageCaption">Making a prediction relative to a sliding window.</figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="e5c3" id="e5c3" class="graf graf--p graf-after--figure">This concept is very similar to the anchors in Faster R-CNN. However, single shot detectors predict both the boundary box and the class at the same time. Let’s do a quick recap. For example, we have an 8 × 8 feature map and we make <strong class="markup--strong markup--p-strong">k</strong> predictions at each location. i.e. 8 × 8 × k predictions.</p><figure name="8b81" id="8b81" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 336px; max-height: 375px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 111.60000000000001%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*i2egSyxtuJo3YYjdLbaBGQ.png" data-width="336" data-height="375"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*i2egSyxtuJo3YYjdLbaBGQ.png?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*i2egSyxtuJo3YYjdLbaBGQ.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*i2egSyxtuJo3YYjdLbaBGQ.png"></noscript></div></div><figcaption class="imageCaption">64 locations</figcaption></figure><p name="75ad" id="75ad" class="graf graf--p graf-after--figure">At each location, we have k anchors (anchors are just fixed initial boundary box guesses), one anchor for one specific prediction. We select the anchors carefully and every location uses the same anchor shapes.</p><figure name="91f9" id="91f9" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 335px; max-height: 337px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 100.6%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*1F8rWQyBV-P8pDn0Avx-OA.png" data-width="335" data-height="337"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*1F8rWQyBV-P8pDn0Avx-OA.png?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*1F8rWQyBV-P8pDn0Avx-OA.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*1F8rWQyBV-P8pDn0Avx-OA.png"></noscript></div></div><figcaption class="imageCaption">Use 4 anchors to make 4 predictions per location.</figcaption></figure><p name="8e35" id="8e35" class="graf graf--p graf-after--figure">Here are 4 anchors (green) and 4 corresponding predictions (blue) each related to one specific anchor.</p><figure name="050d" id="050d" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 350px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 50%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*TjZ-YFE1YLPNOJJzjyFCEQ.jpeg" data-width="1123" data-height="561" data-action="zoom" data-action-value="1*TjZ-YFE1YLPNOJJzjyFCEQ.jpeg"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*TjZ-YFE1YLPNOJJzjyFCEQ.jpeg?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*TjZ-YFE1YLPNOJJzjyFCEQ.jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*TjZ-YFE1YLPNOJJzjyFCEQ.jpeg"></noscript></div></div><figcaption class="imageCaption">4 predictions each relative to an anchor</figcaption></figure><p name="c5c8" id="c5c8" class="graf graf--p graf-after--figure">In Faster R-CNN, we use one convolution filter to make a 5-parameter prediction: 4 parameters on the predicted box relative to an anchor and 1 objectness confidence score. So the 3× 3× D × 5 convolution filter transforms the feature maps from 8 × 8 × D to 8 × 8 × 5.</p><figure name="60cc" id="60cc" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 378px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 54%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*yrAA9xnL4OlhX6RoeHQVtQ.png" data-width="708" data-height="382" data-action="zoom" data-action-value="1*yrAA9xnL4OlhX6RoeHQVtQ.png"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*yrAA9xnL4OlhX6RoeHQVtQ.png?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*yrAA9xnL4OlhX6RoeHQVtQ.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*yrAA9xnL4OlhX6RoeHQVtQ.png"></noscript></div></div><figcaption class="imageCaption">Compute a prediction using a 3x3 convolution filter.</figcaption></figure><p name="1ec5" id="1ec5" class="graf graf--p graf-after--figure">In a single shot detector, the convolution filter also predicts <strong class="markup--strong markup--p-strong">C</strong> class probabilities for classification (one per class). So we apply a 3× 3× D × 25 convolution filter to transform the feature maps from 8 × 8 × D to 8 × 8 × 25 for C=20.</p><figure name="eea5" id="eea5" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 318px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 45.5%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*UsqjfoW3sLkmyXKQ0Hyo8A.png" data-width="1047" data-height="476" data-action="zoom" data-action-value="1*UsqjfoW3sLkmyXKQ0Hyo8A.png"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*UsqjfoW3sLkmyXKQ0Hyo8A.png?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*UsqjfoW3sLkmyXKQ0Hyo8A.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*UsqjfoW3sLkmyXKQ0Hyo8A.png"></noscript></div></div><figcaption class="imageCaption">Each location makes k predictions each have 25 parameters.</figcaption></figure><p name="e998" id="e998" class="graf graf--p graf-after--figure">Single shot detector often trades accuracy with real-time processing speed. They also tend to have issues in detecting objects that are too close or too small. For the picture below, there are 9 Santas in the lower left corner but one of the single shot detectors detects 5 only.</p><figure name="1455" id="1455" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 428px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 61.1%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*j4PnWfxP3yoVPOFyI27tww.jpeg" data-width="3036" data-height="1855" data-action="zoom" data-action-value="1*j4PnWfxP3yoVPOFyI27tww.jpeg"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*j4PnWfxP3yoVPOFyI27tww.jpeg?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*j4PnWfxP3yoVPOFyI27tww.jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*j4PnWfxP3yoVPOFyI27tww.jpeg"></noscript></div></div></figure><h3 name="c48f" id="c48f" class="graf graf--h3 graf-after--figure">SSD</h3><p name="e8fb" id="e8fb" class="graf graf--p graf-after--h3">SSD is a single shot detector using a VGG16 network as a feature extractor (equivalent to the CNN in Faster R-CNN). Then we add custom convolution layers (blue) afterward and use convolution filters (green) to make predictions.</p></div><div class="section-inner sectionLayout--outsetColumn"><figure name="5902" id="5902" class="graf graf--figure graf--layoutOutsetCenter graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1032px; max-height: 93px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 9%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*1C5hgYTdBvCdCYWbXEaVww.png" data-width="2182" data-height="196" data-action="zoom" data-action-value="1*1C5hgYTdBvCdCYWbXEaVww.png"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*1C5hgYTdBvCdCYWbXEaVww.png?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/2400/1*1C5hgYTdBvCdCYWbXEaVww.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/2400/1*1C5hgYTdBvCdCYWbXEaVww.png"></noscript></div></div><figcaption class="imageCaption">Single shot prediction for both classification and location.</figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="73de" id="73de" class="graf graf--p graf-after--figure">However, convolution layers reduce spatial dimension and resolution. So the model above can detect large objects only. To fix that, we make independent object detections from multiple feature maps.</p></div><div class="section-inner sectionLayout--outsetColumn"><figure name="0de0" id="0de0" class="graf graf--figure graf--layoutOutsetCenter graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1032px; max-height: 225px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 21.8%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*k0eFZw1jlF9xPvhzBKt6LQ.png" data-width="1419" data-height="310" data-action="zoom" data-action-value="1*k0eFZw1jlF9xPvhzBKt6LQ.png"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*k0eFZw1jlF9xPvhzBKt6LQ.png?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/2400/1*k0eFZw1jlF9xPvhzBKt6LQ.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/2400/1*k0eFZw1jlF9xPvhzBKt6LQ.png"></noscript></div></div><figcaption class="imageCaption">Use multi-scale feature maps for detection.</figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="db27" id="db27" class="graf graf--p graf-after--figure">Here is the diagram showing the dimensions of feature maps.</p></div><div class="section-inner sectionLayout--outsetColumn"><figure name="8103" id="8103" class="graf graf--figure graf--layoutOutsetCenter graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1032px; max-height: 347px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 33.7%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*up-gIJ9rPkHXUGRoqWuULQ.jpeg" data-width="1435" data-height="483" data-action="zoom" data-action-value="1*up-gIJ9rPkHXUGRoqWuULQ.jpeg"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*up-gIJ9rPkHXUGRoqWuULQ.jpeg?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/2400/1*up-gIJ9rPkHXUGRoqWuULQ.jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/2400/1*up-gIJ9rPkHXUGRoqWuULQ.jpeg"></noscript></div></div><figcaption class="imageCaption"><a href="https://arxiv.org/pdf/1512.02325.pdf" data-href="https://arxiv.org/pdf/1512.02325.pdf" class="markup--anchor markup--figure-anchor" rel="noopener nofollow" target="_blank">Source</a></figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="66ab" id="66ab" class="graf graf--p graf-after--figure">SSD uses layers already deep down into the convolutional network to detect objects. If we redraw the diagram closer to scale, we should realize the spatial resolution has dropped significantly and may already miss the opportunity in locating small objects that are too hard to detect in low resolution. If such problem exists, we need to increase the resolution of the input image.</p><figure name="dc75" id="dc75" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 451px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 64.5%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*oCaj3OQIbhOGSxcgvONTQw.png" data-width="1626" data-height="1048" data-action="zoom" data-action-value="1*oCaj3OQIbhOGSxcgvONTQw.png"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*oCaj3OQIbhOGSxcgvONTQw.png?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*oCaj3OQIbhOGSxcgvONTQw.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*oCaj3OQIbhOGSxcgvONTQw.png"></noscript></div></div></figure><p name="3c1e" id="3c1e" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">YOLO</strong></p><p name="8f09" id="8f09" class="graf graf--p graf-after--p">YOLO is another single shot detector.</p><figure name="da74" id="da74" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.2%;"></div><div class="progressiveMedia js-progressiveMedia"><img src="https://i.embed.ly/1/display/resize?url=https%3A%2F%2Fi.ytimg.com%2Fvi%2FVOC3huqHrss%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;width=40" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><div class="iframeContainer"><IFRAME data-width="854" data-height="480" width="700" height="393" data-src="/media/b901faec3876bb5da6091d9410fd9c72?postId=3888677c5f4d" data-media-id="b901faec3876bb5da6091d9410fd9c72" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Fi.ytimg.com%2Fvi%2FVOC3huqHrss%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen frameborder="0"></IFRAME></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME data-width="854" data-height="480" width="700" height="393" src="/media/b901faec3876bb5da6091d9410fd9c72?postId=3888677c5f4d" data-media-id="b901faec3876bb5da6091d9410fd9c72" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Fi.ytimg.com%2Fvi%2FVOC3huqHrss%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div><figcaption class="imageCaption">Object detection in real-time</figcaption></figure><p name="e733" id="e733" class="graf graf--p graf-after--figure">YOLO uses DarkNet to make feature detection followed by convolutional layers.</p></div><div class="section-inner sectionLayout--outsetColumn"><figure name="8ac0" id="8ac0" class="graf graf--figure graf--layoutOutsetCenter graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1032px; max-height: 121px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 11.799999999999999%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*NBnDpz8fitkhcdnkgF2bvg.png" data-width="1794" data-height="211" data-action="zoom" data-action-value="1*NBnDpz8fitkhcdnkgF2bvg.png"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*NBnDpz8fitkhcdnkgF2bvg.png?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/2400/1*NBnDpz8fitkhcdnkgF2bvg.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/2400/1*NBnDpz8fitkhcdnkgF2bvg.png"></noscript></div></div></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="f5ff" id="f5ff" class="graf graf--p graf-after--figure">However, it does not make independent detections using multi-scale feature maps. Instead, it partially flattens features maps and concatenates it with another lower resolution maps. For example, YOLO reshapes a 28 × 28 × 512 layer to 14 × 14 × 2048. Then it concatenates with the 14 × 14 ×1024 feature maps. Afterward, YOLO applies convolution filters on the new 14 × 14 × 3072 layer to make predictions.</p><p name="a4f2" id="a4f2" class="graf graf--p graf-after--p">YOLO (v2) makes many implementation improvements to push the mAP from 63.4 for the first release to 78.6. YOLO9000 can detect 9000 different categories of objects.</p><figure name="e4b5" id="e4b5" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 289px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 41.3%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*3IdCKSzR5R0lIE1LSmN4Bg.png" data-width="1568" data-height="648" data-action="zoom" data-action-value="1*3IdCKSzR5R0lIE1LSmN4Bg.png"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*3IdCKSzR5R0lIE1LSmN4Bg.png?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*3IdCKSzR5R0lIE1LSmN4Bg.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*3IdCKSzR5R0lIE1LSmN4Bg.png"></noscript></div></div><figcaption class="imageCaption"><a href="https://arxiv.org/pdf/1612.08242.pdf" data-href="https://arxiv.org/pdf/1612.08242.pdf" class="markup--anchor markup--figure-anchor" rel="noopener nofollow" target="_blank">Source</a></figcaption></figure><p name="ba43" id="ba43" class="graf graf--p graf-after--figure">Here are the mAP and FPS comparison for different detectors reported by the YOLO paper. YOLOv2 can take different input image resolutions. Lower resolution input images achieve higher FPS but lower mAP.</p><figure name="4e7e" id="4e7e" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 462px; max-height: 325px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 70.3%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*NJj17Z6FgffYaA4WH2WIjw.png" data-width="462" data-height="325"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*NJj17Z6FgffYaA4WH2WIjw.png?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*NJj17Z6FgffYaA4WH2WIjw.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*NJj17Z6FgffYaA4WH2WIjw.png"></noscript></div></div><figcaption class="imageCaption"><a href="https://arxiv.org/pdf/1612.08242.pdf" data-href="https://arxiv.org/pdf/1612.08242.pdf" class="markup--anchor markup--figure-anchor" rel="noopener nofollow" target="_blank">Source</a></figcaption></figure><p name="f271" id="f271" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">YOLOv3</strong></p><p name="d3ee" id="d3ee" class="graf graf--p graf-after--p">YOLOv3 change to a more complex backbone for feature extraction. Darknet-53 mainly compose of 3 × 3 and 1× 1 filters with skip connections like the residual network in ResNet. Darknet-53 has less BFLOP (billion floating point operations) than ResNet-152, but achieves the same classification accuracy at 2x faster.</p><figure name="f720" id="f720" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 334px; max-height: 462px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 138.3%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*biRYJyCSv-UTbTQTa4Afqg.png" data-width="334" data-height="462"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*biRYJyCSv-UTbTQTa4Afqg.png?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*biRYJyCSv-UTbTQTa4Afqg.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*biRYJyCSv-UTbTQTa4Afqg.png"></noscript></div></div><figcaption class="imageCaption"><a href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" data-href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">Source</a></figcaption></figure><p name="d540" id="d540" class="graf graf--p graf-after--figure">YOLOv3 also adds Feature Pyramid (discussed next) to detect small objects better. Here is the accuracy and speed tradeoff for different detectors.</p><figure name="81d1" id="81d1" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 436px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 62.3%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*rfj_5yjKZm2LJvVzMXmLFA.png" data-width="800" data-height="498" data-action="zoom" data-action-value="1*rfj_5yjKZm2LJvVzMXmLFA.png"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*rfj_5yjKZm2LJvVzMXmLFA.png?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*rfj_5yjKZm2LJvVzMXmLFA.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*rfj_5yjKZm2LJvVzMXmLFA.png"></noscript></div></div><figcaption class="imageCaption"><a href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" data-href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" class="markup--anchor markup--figure-anchor" rel="noopener nofollow" target="_blank">Source</a></figcaption></figure><h3 name="bcfc" id="bcfc" class="graf graf--h3 graf-after--figure">Feature Pyramid Networks (FPN)</h3><p name="b4fb" id="b4fb" class="graf graf--p graf-after--h3">Detecting objects in different scales is challenging in particular for small objects. Feature Pyramid Network (<strong class="markup--strong markup--p-strong">FPN</strong>) is a feature extractor designed with feature pyramid concept to improve accuracy and speed. It replaces the feature extractor of detectors like Faster R-CNN and generates higher quality feature map pyramid.</p><p name="5a42" id="5a42" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Data Flow</strong></p><figure name="5f14" id="5f14" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 500px; max-height: 201px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 40.2%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*aMRoAN7CtD1gdzTaZIT5gA.png" data-width="500" data-height="201"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*aMRoAN7CtD1gdzTaZIT5gA.png?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*aMRoAN7CtD1gdzTaZIT5gA.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*aMRoAN7CtD1gdzTaZIT5gA.png"></noscript></div></div><figcaption class="imageCaption">FPN (<a href="https://arxiv.org/pdf/1612.03144.pdf" data-href="https://arxiv.org/pdf/1612.03144.pdf" class="markup--anchor markup--figure-anchor" rel="noopener nofollow" target="_blank">Source</a>)</figcaption></figure><p name="c11c" id="c11c" class="graf graf--p graf-after--figure">FPN composes of a <strong class="markup--strong markup--p-strong">bottom-up</strong> and a <strong class="markup--strong markup--p-strong">top-down</strong> pathway. The bottom-up pathway is the usual convolutional network for feature extraction. As we go up, the spatial resolution decreases. With more high-level structures detected, the <strong class="markup--strong markup--p-strong">semantic value</strong> for each layer increases.</p><figure name="9030" id="9030" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 470px; max-height: 318px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 67.7%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*_kxgFskpRJ6bsxEjh9CH6g.jpeg" data-width="470" data-height="318"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*_kxgFskpRJ6bsxEjh9CH6g.jpeg?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*_kxgFskpRJ6bsxEjh9CH6g.jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*_kxgFskpRJ6bsxEjh9CH6g.jpeg"></noscript></div></div><figcaption class="imageCaption">Feature extraction in FPN (Modified from <a href="https://arxiv.org/pdf/1612.03144.pdf" data-href="https://arxiv.org/pdf/1612.03144.pdf" class="markup--anchor markup--figure-anchor" rel="noopener nofollow" target="_blank">source</a>)</figcaption></figure><p name="ff43" id="ff43" class="graf graf--p graf-after--figure">SSD makes detection from multiple feature maps. However, the bottom layers are not selected for object detection. They are in high resolution but the semantic value is not high enough to justify its use as the speed slow-down is significant. So SSD only uses upper layers for detection and therefore performs much worse for small objects.</p><figure name="3038" id="3038" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 400px; max-height: 246px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 61.5%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*M_c6Jx5Uy7qr6vJbrtAvhg.png" data-width="400" data-height="246"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*M_c6Jx5Uy7qr6vJbrtAvhg.png?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*M_c6Jx5Uy7qr6vJbrtAvhg.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*M_c6Jx5Uy7qr6vJbrtAvhg.png"></noscript></div></div><figcaption class="imageCaption">Modified from <a href="https://arxiv.org/pdf/1612.03144.pdf" data-href="https://arxiv.org/pdf/1612.03144.pdf" class="markup--anchor markup--figure-anchor" rel="noopener nofollow" target="_blank">source</a></figcaption></figure><p name="de21" id="de21" class="graf graf--p graf-after--figure">FPN provides a top-down pathway to construct higher resolution layers from a semantic rich layer.</p><figure name="f1f7" id="f1f7" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 500px; max-height: 201px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 40.2%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*XmNDHT8WWZbXACyBjg3ZeQ.jpeg" data-width="500" data-height="201"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*XmNDHT8WWZbXACyBjg3ZeQ.jpeg?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*XmNDHT8WWZbXACyBjg3ZeQ.jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*XmNDHT8WWZbXACyBjg3ZeQ.jpeg"></noscript></div></div><figcaption class="imageCaption">Reconstruct spatial resolution in the top-down pathway. (Modified from <a href="https://arxiv.org/pdf/1612.03144.pdf" data-href="https://arxiv.org/pdf/1612.03144.pdf" class="markup--anchor markup--figure-anchor" rel="noopener nofollow" target="_blank">source</a>)</figcaption></figure><p name="fc73" id="fc73" class="graf graf--p graf-after--figure">While the reconstructed layers are semantic strong but the locations of objects are not precise after all the downsampling and upsampling. We add lateral connections between reconstructed layers and the corresponding feature maps to help the detector to predict the location betters.</p><figure name="3fb5" id="3fb5" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 500px; max-height: 201px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 40.2%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*aMRoAN7CtD1gdzTaZIT5gA.png" data-width="500" data-height="201"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*aMRoAN7CtD1gdzTaZIT5gA.png?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*aMRoAN7CtD1gdzTaZIT5gA.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*aMRoAN7CtD1gdzTaZIT5gA.png"></noscript></div></div><figcaption class="imageCaption">Add skip connections (<a href="https://arxiv.org/pdf/1612.03144.pdf" data-href="https://arxiv.org/pdf/1612.03144.pdf" class="markup--anchor markup--figure-anchor" rel="noopener nofollow" target="_blank">Source</a>)</figcaption></figure><p name="8a0e" id="8a0e" class="graf graf--p graf-after--figure">The following is a detail diagram on the bottom-up and the top-down pathway. P2, P3, P4 and P5 are the pyramid of feature maps for object detection.</p><figure name="0ae5" id="0ae5" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 820px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 117.10000000000001%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*ffxP_rL8-jMvipLhMJrVeA.png" data-width="968" data-height="1134" data-action="zoom" data-action-value="1*ffxP_rL8-jMvipLhMJrVeA.png"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*ffxP_rL8-jMvipLhMJrVeA.png?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*ffxP_rL8-jMvipLhMJrVeA.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*ffxP_rL8-jMvipLhMJrVeA.png"></noscript></div></div></figure><p name="9592" id="9592" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">FPN with RPN</strong></p><p name="4d45" id="4d45" class="graf graf--p graf-after--p">FPN is not an object detector by itself. It is a feature detector that works with object detectors. The following feed each feature maps (P2 to P5) independently to make object detection.</p><figure name="7301" id="7301" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 497px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 71%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*cHR4YRqdPBOx4IDqzU-GwQ.png" data-width="1602" data-height="1138" data-action="zoom" data-action-value="1*cHR4YRqdPBOx4IDqzU-GwQ.png"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*cHR4YRqdPBOx4IDqzU-GwQ.png?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*cHR4YRqdPBOx4IDqzU-GwQ.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*cHR4YRqdPBOx4IDqzU-GwQ.png"></noscript></div></div></figure><p name="5810" id="5810" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">FPN with Fast R-CNN or Faster R-CNN</strong></p><p name="635d" id="635d" class="graf graf--p graf-after--p">In FPN, we generate a pyramid of feature maps. We apply the RPN (described above) to generate ROIs. Based on the size of the ROI, we select the feature map layer in the most proper scale to extract the feature patches.</p></div><div class="section-inner sectionLayout--outsetColumn"><figure name="775e" id="775e" class="graf graf--figure graf--layoutOutsetCenter graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1000px; max-height: 368px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 36.8%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*Wvn0WG4XZ0w9Ed2fFYPrXw.jpeg" data-width="1000" data-height="368"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*Wvn0WG4XZ0w9Ed2fFYPrXw.jpeg?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/2400/1*Wvn0WG4XZ0w9Ed2fFYPrXw.jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/2400/1*Wvn0WG4XZ0w9Ed2fFYPrXw.jpeg"></noscript></div></div></figure></div><div class="section-inner sectionLayout--insetColumn"><h3 name="ccd4" id="ccd4" class="graf graf--h3 graf-after--figure"><strong class="markup--strong markup--h3-strong">Hard example mining</strong></h3><p name="d34b" id="d34b" class="graf graf--p graf-after--h3">For most detectors like SSD and YOLO, we make far more predictions than the number of objects presence. So there are much more negative matches than positive matches. This creates a class imbalance which hurts training. We are training the model to learn background space rather than detecting objects. However, we need negative sampling so it can learn what constitutes a bad prediction. So, for example in SSD, we sort training examples by their calculated confidence loss. We pick the top ones and makes sure the ratio between the picked negatives and positives is at most 3:1. This leads to a faster and more stable training.</p><h3 name="5c31" id="5c31" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Non-maximal suppression in inference</strong></h3><p name="9c17" id="9c17" class="graf graf--p graf-after--h3">Detectors can make duplicate detections for the same object. To fix this, we apply non-maximal suppression to remove duplications with lower confidence. We sort the predictions by the confidence scores and go down the list one by one. If any previous prediction has the same class and IoU greater than 0.5 with the current prediction, we remove it from the list.</p><h3 name="0f20" id="0f20" class="graf graf--h3 graf-after--p">Focal loss (RetinaNet)</h3><p name="970c" id="970c" class="graf graf--p graf-after--h3">Class imbalance hurts performance. SSD resamples the ratio of the object class and background class during training so it will not be overwhelmed by image background. Focal loss (FL) adopts another approach to reduce loss for well-trained class. So whenever the model is good at detecting background, it will reduce its loss and reemphasize the training on the object class. We start with the cross-entropy loss CE and we add a weight to de-emphasize the CE for high confidence class.</p><figure name="e8db" id="e8db" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 178px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 25.4%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*DgI0JPk98eXfLvmkK_9PGQ.png" data-width="1486" data-height="378" data-action="zoom" data-action-value="1*DgI0JPk98eXfLvmkK_9PGQ.png"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*DgI0JPk98eXfLvmkK_9PGQ.png?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*DgI0JPk98eXfLvmkK_9PGQ.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*DgI0JPk98eXfLvmkK_9PGQ.png"></noscript></div></div></figure><p name="0305" id="0305" class="graf graf--p graf-after--figure">For example, for γ = 0.5, the focal loss for well-classified examples will be pushed toward 0.</p><figure name="db52" id="db52" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 400px; max-height: 240px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 60%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*FCV96tP679EScoiwKq4IaQ.png" data-width="400" data-height="240"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*FCV96tP679EScoiwKq4IaQ.png?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*FCV96tP679EScoiwKq4IaQ.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*FCV96tP679EScoiwKq4IaQ.png"></noscript></div></div><figcaption class="imageCaption">Modified from <a href="https://arxiv.org/pdf/1708.02002.pdf" data-href="https://arxiv.org/pdf/1708.02002.pdf" class="markup--anchor markup--figure-anchor" rel="noopener nofollow" target="_blank">source</a>.</figcaption></figure><p name="4717" id="4717" class="graf graf--p graf-after--figure">Here is the RetinaNet building on FPN and ResNet using the Focal loss.</p></div><div class="section-inner sectionLayout--outsetColumn"><figure name="4da3" id="4da3" class="graf graf--figure graf--layoutOutsetCenter graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1032px; max-height: 276px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 26.8%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*jQFeF7gj6uCXVzUb08S9lg.png" data-width="1906" data-height="510" data-action="zoom" data-action-value="1*jQFeF7gj6uCXVzUb08S9lg.png"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*jQFeF7gj6uCXVzUb08S9lg.png?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/2400/1*jQFeF7gj6uCXVzUb08S9lg.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/2400/1*jQFeF7gj6uCXVzUb08S9lg.png"></noscript></div></div><figcaption class="imageCaption"><a href="https://arxiv.org/pdf/1708.02002.pdf" data-href="https://arxiv.org/pdf/1708.02002.pdf" class="markup--anchor markup--figure-anchor" rel="noopener nofollow" target="_blank">RetinaNet</a></figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><h3 name="3813" id="3813" class="graf graf--h3 graf-after--figure">Further reading on SSD, YOLO &amp; FPN</h3><p name="dcf5" id="dcf5" class="graf graf--p graf-after--h3">Both SSD and YOLO are more complex than we described here. For further study, please refer to:</p><ul class="postList"><li name="74ba" id="74ba" class="graf graf--li graf-after--p"><a href="https://medium.com/@jonathan_hui/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06" data-href="https://medium.com/@jonathan_hui/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06" class="markup--anchor markup--li-anchor" target="_blank">SSD object detection</a>.</li><li name="2669" id="2669" class="graf graf--li graf-after--li"><a href="https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088" data-href="https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088" class="markup--anchor markup--li-anchor" target="_blank">YOLO object detection</a>.</li><li name="21cb" id="21cb" class="graf graf--li graf-after--li"><a href="https://medium.com/@jonathan_hui/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c" data-href="https://medium.com/@jonathan_hui/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c" class="markup--anchor markup--li-anchor" target="_blank">FPN object detection</a>.</li></ul><h3 name="528e" id="528e" class="graf graf--h3 graf-after--li">Resources</h3><p name="7e93" id="7e93" class="graf graf--p graf-after--h3 graf--trailing">Please refer to the <a href="https://medium.com/@jonathan_hui/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06" data-href="https://medium.com/@jonathan_hui/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06" class="markup--anchor markup--p-anchor" target="_blank">SSD</a> and <a href="https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088" data-href="https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088" class="markup--anchor markup--p-anchor" target="_blank">YOLO</a> article for the corresponding implementations.</p></div></div></section></div><footer class="u-paddingTop10"><div class="container u-maxWidth740"><div class="row"><div class="col u-size12of12"></div></div><div class="row"><div class="col u-size12of12 js-postTags"><div class="u-paddingBottom10"><ul class="tags tags--postTags tags--borderless"><li><a class="link u-baseColor--link"   href="https://medium.com/tag/machine-learning?source=post" data-action-source="post">Machine Learning</a></li><li><a class="link u-baseColor--link"   href="https://medium.com/tag/deep-learning?source=post" data-action-source="post">Deep Learning</a></li><li><a class="link u-baseColor--link"   href="https://medium.com/tag/computer-vision?source=post" data-action-source="post">Computer Vision</a></li><li><a class="link u-baseColor--link"   href="https://medium.com/tag/object-detection?source=post" data-action-source="post">Object Detection</a></li><li><a class="link u-baseColor--link"   href="https://medium.com/tag/artificial-intelligence?source=post" data-action-source="post">Artificial Intelligence</a></li></ul></div></div></div><div class="postActions js-postActionsFooter "><div class="u-flexCenter"><div class="u-flex1"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="3888677c5f4d" data-is-icon-29px="true" data-is-circle="true" data-has-recommend-list="true" data-source="post_actions_footer-----3888677c5f4d---------------------clap_footer" data-clap-string-singular="clap" data-clap-string-plural="claps"><div class="u-relative u-foreground"><button class="button button--large button--circle button--withChrome u-baseColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker clapButton--largePill u-relative u-foreground u-xs-paddingLeft13 u-width60 u-height60 u-accentColor--textNormal u-accentColor--buttonNormal clap-onboarding"  data-action="sign-up-prompt" data-sign-in-action="multivote" data-requires-token="true" data-redirect="https://medium.com/_/vote/p/3888677c5f4d" data-action-source="post_actions_footer-----3888677c5f4d---------------------clap_footer" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--33px u-relative u-topNegative2 u-xs-top0"><svg class="svgIcon-use" width="33" height="33" ><path d="M28.86 17.342l-3.64-6.402c-.292-.433-.712-.729-1.163-.8a1.124 1.124 0 0 0-.889.213c-.63.488-.742 1.181-.33 2.061l1.222 2.587 1.4 2.46c2.234 4.085 1.511 8.007-2.145 11.663-.26.26-.526.49-.797.707 1.42-.084 2.881-.683 4.292-2.094 3.822-3.823 3.565-7.876 2.05-10.395zm-6.252 11.075c3.352-3.35 3.998-6.775 1.978-10.469l-3.378-5.945c-.292-.432-.712-.728-1.163-.8a1.122 1.122 0 0 0-.89.213c-.63.49-.742 1.182-.33 2.061l1.72 3.638a.502.502 0 0 1-.806.568l-8.91-8.91a1.335 1.335 0 0 0-1.887 1.886l5.292 5.292a.5.5 0 0 1-.707.707l-5.292-5.292-1.492-1.492c-.503-.503-1.382-.505-1.887 0a1.337 1.337 0 0 0 0 1.886l1.493 1.492 5.292 5.292a.499.499 0 0 1-.353.854.5.5 0 0 1-.354-.147L5.642 13.96a1.338 1.338 0 0 0-1.887 0 1.338 1.338 0 0 0 0 1.887l2.23 2.228 3.322 3.324a.499.499 0 0 1-.353.853.502.502 0 0 1-.354-.146l-3.323-3.324a1.333 1.333 0 0 0-1.886 0 1.325 1.325 0 0 0-.39.943c0 .356.138.691.39.943l6.396 6.397c3.528 3.53 8.86 5.313 12.821 1.353zM12.73 9.26l5.68 5.68-.49-1.037c-.518-1.107-.426-2.13.224-2.89l-3.303-3.304a1.337 1.337 0 0 0-1.886 0 1.326 1.326 0 0 0-.39.944c0 .217.067.42.165.607zm14.787 19.184c-1.599 1.6-3.417 2.392-5.353 2.392-.349 0-.7-.03-1.058-.082a7.922 7.922 0 0 1-3.667.887c-3.049 0-6.115-1.626-8.359-3.87l-6.396-6.397A2.315 2.315 0 0 1 2 19.724a2.327 2.327 0 0 1 1.923-2.296l-.875-.875a2.339 2.339 0 0 1 0-3.3 2.33 2.33 0 0 1 1.24-.647l-.139-.139c-.91-.91-.91-2.39 0-3.3.884-.884 2.421-.882 3.301 0l.138.14a2.335 2.335 0 0 1 3.948-1.24l.093.092c.091-.423.291-.828.62-1.157a2.336 2.336 0 0 1 3.3 0l3.384 3.386a2.167 2.167 0 0 1 1.271-.173c.534.086 1.03.354 1.441.765.11-.549.415-1.034.911-1.418a2.12 2.12 0 0 1 1.661-.41c.727.117 1.385.565 1.853 1.262l3.652 6.423c1.704 2.832 2.025 7.377-2.205 11.607zM13.217.484l-1.917.882 2.37 2.837-.454-3.719zm8.487.877l-1.928-.86-.44 3.697 2.368-2.837zM16.5 3.293L15.478-.005h2.044L16.5 3.293z" fill-rule="evenodd"/></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--33px u-relative u-topNegative2 u-xs-top0"><svg class="svgIcon-use" width="33" height="33" ><g fill-rule="evenodd"><path d="M29.58 17.1l-3.854-6.78c-.365-.543-.876-.899-1.431-.989a1.491 1.491 0 0 0-1.16.281c-.42.327-.65.736-.7 1.207v.001l3.623 6.367c2.46 4.498 1.67 8.802-2.333 12.807-.265.265-.536.505-.81.728 1.973-.222 3.474-1.286 4.45-2.263 4.166-4.165 3.875-8.6 2.215-11.36zm-4.831.82l-3.581-6.3c-.296-.439-.725-.742-1.183-.815a1.105 1.105 0 0 0-.89.213c-.647.502-.755 1.188-.33 2.098l1.825 3.858a.601.601 0 0 1-.197.747.596.596 0 0 1-.77-.067L10.178 8.21c-.508-.506-1.393-.506-1.901 0a1.335 1.335 0 0 0-.393.95c0 .36.139.698.393.95v.001l5.61 5.61a.599.599 0 1 1-.848.847l-5.606-5.606c-.001 0-.002 0-.003-.002L5.848 9.375a1.349 1.349 0 0 0-1.902 0 1.348 1.348 0 0 0 0 1.901l1.582 1.582 5.61 5.61a.6.6 0 0 1-.848.848l-5.61-5.61c-.51-.508-1.393-.508-1.9 0a1.332 1.332 0 0 0-.394.95c0 .36.139.697.393.952l2.363 2.362c.002.001.002.002.002.003l3.52 3.52a.6.6 0 0 1-.848.847l-3.522-3.523h-.001a1.336 1.336 0 0 0-.95-.393 1.345 1.345 0 0 0-.949 2.295l6.779 6.78c3.715 3.713 9.327 5.598 13.49 1.434 3.527-3.528 4.21-7.13 2.086-11.015zM11.817 7.727c.06-.328.213-.64.466-.893.64-.64 1.755-.64 2.396 0l3.232 3.232c-.82.783-1.09 1.833-.764 2.992l-5.33-5.33z"/><path d="M13.285.48l-1.916.881 2.37 2.837z"/><path d="M21.719 1.361L19.79.501l-.44 3.697z"/><path d="M16.502 3.298L15.481 0h2.043z"/></g></svg></span></span></button><div class="clapUndo u-width60 u-round u-height32 u-absolute u-borderBox u-paddingRight5 u-transition--transform200Springu-backgroundGrayLighter js-clapUndo" style="top: 14px; padding: 2px;"><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon u-floatRight"  data-action="multivote-undo" data-action-value="3888677c5f4d"><span class="svgIcon svgIcon--removeThin svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" ><path d="M20.13 8.11l-5.61 5.61-5.609-5.61-.801.801 5.61 5.61-5.61 5.61.801.8 5.61-5.609 5.61 5.61.8-.801-5.609-5.61 5.61-5.61" fill-rule="evenodd"/></svg></span></button></div></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft16"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-textColorDarker"  data-action="show-recommends" data-action-value="3888677c5f4d">1.97K claps</button><span class="u-xs-hide"></span></span></div></div><div class="buttonSet u-flex0"><a class="button button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon button--dark button--chromeless u-xs-hide u-marginRight12"   href="https://medium.com/p/3888677c5f4d/share/twitter" title="Share on Twitter" aria-label="Share on Twitter" target="_blank" data-action-source="post_actions_footer"><span class="button-defaultState"><span class="svgIcon svgIcon--twitterFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" ><path d="M22.053 7.54a4.474 4.474 0 0 0-3.31-1.455 4.526 4.526 0 0 0-4.526 4.524c0 .35.04.7.082 1.05a12.9 12.9 0 0 1-9.3-4.77c-.39.69-.61 1.46-.65 2.26.03 1.6.83 2.99 2.02 3.79-.72-.02-1.41-.22-2.02-.57-.01.02-.01.04 0 .08-.01 2.17 1.55 4 3.63 4.44-.39.08-.79.13-1.21.16-.28-.03-.57-.05-.81-.08.54 1.77 2.21 3.08 4.2 3.15a9.564 9.564 0 0 1-5.66 1.94c-.34-.03-.7-.06-1.05-.08 2 1.27 4.38 2.02 6.94 2.02 8.31 0 12.86-6.9 12.84-12.85.02-.24.01-.43 0-.65.89-.62 1.65-1.42 2.26-2.34-.82.38-1.69.62-2.59.72a4.37 4.37 0 0 0 1.94-2.51c-.84.53-1.81.9-2.83 1.13z"/></svg></span></span></a><a class="button button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon button--dark button--chromeless u-xs-hide u-marginRight12"   href="https://medium.com/p/3888677c5f4d/share/facebook" title="Share on Facebook" aria-label="Share on Facebook" target="_blank" data-action-source="post_actions_footer"><span class="button-defaultState"><span class="svgIcon svgIcon--facebookSquare svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" ><path d="M23.209 5H5.792A.792.792 0 0 0 5 5.791V23.21c0 .437.354.791.792.791h9.303v-7.125H12.72v-2.968h2.375v-2.375c0-2.455 1.553-3.662 3.741-3.662 1.049 0 1.95.078 2.213.112v2.565h-1.517c-1.192 0-1.469.567-1.469 1.397v1.963h2.969l-.594 2.968h-2.375L18.11 24h5.099a.791.791 0 0 0 .791-.791V5.79a.791.791 0 0 0-.791-.79"/></svg></span></span></a><button class="button button--large button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon u-xs-show u-marginRight10"  title="Share this story on Twitter or Facebook" aria-label="Share this story on Twitter or Facebook" data-action="show-share-popover" data-action-source="post_actions_footer"><span class="svgIcon svgIcon--share svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" ><path d="M20.385 8H19a.5.5 0 1 0 .011 1h1.39c.43 0 .84.168 1.14.473.31.305.48.71.48 1.142v10.77c0 .43-.17.837-.47 1.142-.3.305-.71.473-1.14.473H8.62c-.43 0-.84-.168-1.144-.473a1.603 1.603 0 0 1-.473-1.142v-10.77c0-.43.17-.837.48-1.142A1.599 1.599 0 0 1 8.62 9H10a.502.502 0 0 0 0-1H8.615c-.67 0-1.338.255-1.85.766-.51.51-.765 1.18-.765 1.85v10.77c0 .668.255 1.337.766 1.848.51.51 1.18.766 1.85.766h11.77c.668 0 1.337-.255 1.848-.766.51-.51.766-1.18.766-1.85v-10.77c0-.668-.255-1.337-.766-1.848A2.61 2.61 0 0 0 20.384 8zm-8.67-2.508L14 3.207v8.362c0 .27.224.5.5.5s.5-.23.5-.5V3.2l2.285 2.285a.49.49 0 0 0 .704-.001.511.511 0 0 0 0-.708l-3.14-3.14a.504.504 0 0 0-.71 0L11 4.776a.501.501 0 0 0 .71.706" fill-rule="evenodd"/></svg></span></button><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon"  data-action="scroll-to-responses" data-action-source="post_actions_footer"><span class="svgIcon svgIcon--response svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" ><path d="M21.27 20.058c1.89-1.826 2.754-4.17 2.754-6.674C24.024 8.21 19.67 4 14.1 4 8.53 4 4 8.21 4 13.384c0 5.175 4.53 9.385 10.1 9.385 1.007 0 2-.14 2.95-.41.285.25.592.49.918.7 1.306.87 2.716 1.31 4.19 1.31.276-.01.494-.14.6-.36a.625.625 0 0 0-.052-.65c-.61-.84-1.042-1.71-1.282-2.58a5.417 5.417 0 0 1-.154-.75zm-3.85 1.324l-.083-.28-.388.12a9.72 9.72 0 0 1-2.85.424c-4.96 0-8.99-3.706-8.99-8.262 0-4.556 4.03-8.263 8.99-8.263 4.95 0 8.77 3.71 8.77 8.27 0 2.25-.75 4.35-2.5 5.92l-.24.21v.32c0 .07 0 .19.02.37.03.29.1.6.19.92.19.7.49 1.4.89 2.08-.93-.14-1.83-.49-2.67-1.06-.34-.22-.88-.48-1.16-.74z"/></svg></span></button><button class="button button--chromeless u-baseColor--buttonNormal u-marginRight12"  data-action="scroll-to-responses">13</button><button class="button button--large button--dark button--chromeless is-touchIconFadeInPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton"  title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="sign-up-prompt" data-sign-in-action="add-to-bookmarks" data-requires-token="true" data-redirect="https://medium.com/_/bookmark/p/3888677c5f4d" data-action-source="post_actions_footer"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--29px u-marginRight4"><svg class="svgIcon-use" width="29" height="29" ><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4zM21 23l-5.91-3.955-.148-.107a.751.751 0 0 0-.884 0l-.147.107L8 23V6.615C8 5.725 8.725 5 9.615 5h9.77C20.275 5 21 5.725 21 6.615V23z" fill-rule="evenodd"/></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--29px u-marginRight4"><svg class="svgIcon-use" width="29" height="29" ><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4z" fill-rule="evenodd"/></svg></span></span></button><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon js-moreActionsButton"  title="More actions" aria-label="More actions" data-action="more-actions"><span class="svgIcon svgIcon--more svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"  viewBox="-480.5 272.5 21 21"><path d="M-463 284.6c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5z"/></svg></span></button></div></div></div></div><div class="u-maxWidth740 u-paddingTop20 u-marginTop20 u-borderTopLightest container u-paddingBottom20 u-xs-paddingBottom10 js-postAttributionFooterContainer"><div class="row js-postFooterInfo"><div class="col u-size12of12"><li class="uiScale uiScale-ui--small uiScale-caption--regular u-block u-paddingBottom18 js-cardUser"><div class="u-marginLeft20 u-floatRight"><span class="followState js-followState" data-user-id="bd51f1a63813"><button class="button button--small u-noUserSelect button--withChrome u-baseColor--buttonNormal button--withHover button--unblock js-unblockButton"  data-action="sign-up-prompt" data-sign-in-action="toggle-block-user" data-requires-token="true" data-redirect="https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d" data-action-source="footer_card"><span class="button-label  button-defaultState">Blocked</span><span class="button-label button-hoverState">Unblock</span></button><button class="button button--primary button--small u-noUserSelect button--withChrome u-accentColor--buttonNormal button--follow js-followButton"  data-action="sign-up-prompt" data-sign-in-action="toggle-subscribe-user" data-requires-token="true" data-redirect="https://medium.com/_/subscribe/user/bd51f1a63813" data-action-source="footer_card-bd51f1a63813-------------------------follow_footer"><span class="button-label  button-defaultState js-buttonLabel">Follow</span><span class="button-label button-activeState">Following</span></button></span></div><div class="u-tableCell"><a class="link u-baseColor--link avatar"   href="https://medium.com/@jonathan_hui?source=footer_card" title="Go to the profile of Jonathan Hui" aria-label="Go to the profile of Jonathan Hui" data-action-source="footer_card" data-user-id="bd51f1a63813" dir="auto"><img  src="https://cdn-images-1.medium.com/fit/c/120/120/1*c3Z3aOPBooxEX4tx4RkzLw.jpeg" class="avatar-image avatar-image--small" alt="Go to the profile of Jonathan Hui"></a></div><div class="u-tableCell u-verticalAlignMiddle u-breakWord u-paddingLeft15"><h3 class="ui-h3 u-fontSize18 u-lineHeightTighter u-marginBottom4"><a class="link link--primary u-accentColor--hoverTextNormal"   href="https://medium.com/@jonathan_hui" property="cc:attributionName" title="Go to the profile of Jonathan Hui" aria-label="Go to the profile of Jonathan Hui" rel="author cc:attributionUrl" data-user-id="bd51f1a63813" dir="auto">Jonathan Hui</a></h3><p class="ui-body u-fontSize14 u-lineHeightBaseSans u-textColorDark u-marginBottom4">Deep Learning</p></div></li></div></div></div><div class="js-postFooterPlacements" data-post-id="3888677c5f4d"></div><div class="u-padding0 u-clearfix u-backgroundGrayLightest u-print-hide supplementalPostContent js-responsesWrapper"></div><div class="supplementalPostContent js-heroPromo"></div></footer></article></main><aside class="u-marginAuto u-maxWidth1032 js-postLeftSidebar"><div class="u-foreground u-top0 u-transition--fadeOut300 u-fixed u-sm-hide js-postShareWidget"><ul><li class="u-marginVertical10"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="3888677c5f4d" data-is-icon-29px="true" data-has-recommend-list="true" data-source="post_share_widget-----3888677c5f4d---------------------clap_sidebar"><div class="u-relative u-foreground"><button class="button button--primary button--large button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker"  data-action="sign-up-prompt" data-sign-in-action="multivote" data-requires-token="true" data-redirect="https://medium.com/_/vote/p/3888677c5f4d" data-action-source="post_share_widget-----3888677c5f4d---------------------clap_sidebar" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" ><g fill-rule="evenodd"><path d="M13.739 1l.761 2.966L15.261 1z"/><path d="M16.815 4.776l1.84-2.551-1.43-.471z"/><path d="M10.378 2.224l1.84 2.551-.408-3.022z"/><path d="M22.382 22.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L6.11 15.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L8.43 9.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L20.628 15c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM12.99 6.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"/></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" ><g fill-rule="evenodd"><path d="M13.738 1l.762 2.966L15.262 1z"/><path d="M18.634 2.224l-1.432-.47-.408 3.022z"/><path d="M11.79 1.754l-1.431.47 1.84 2.552z"/><path d="M24.472 14.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"/><path d="M14.58 10.887c-.156-.83.096-1.569.692-2.142L12.78 6.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"/><path d="M17.812 10.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L9.2 7.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L7.046 9.54 5.802 8.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394l1.241 1.241 4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L4.89 11.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C21.74 20.8 22.271 18 20.62 14.982l-2.809-4.942z"/></g></svg></span></span></button></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton"  data-action="show-recommends" data-action-value="3888677c5f4d">1.97K</button></span></div></li><li class="u-marginVertical10 u-marginLeft3"><button class="button button--large button--dark button--chromeless is-touchIconFadeInPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton"  title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="sign-up-prompt" data-sign-in-action="add-to-bookmarks" data-requires-token="true" data-redirect="https://medium.com/_/bookmark/p/3888677c5f4d" data-action-source="post_share_widget-----3888677c5f4d---------------------bookmark_sidebar"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" ><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4zM21 23l-5.91-3.955-.148-.107a.751.751 0 0 0-.884 0l-.147.107L8 23V6.615C8 5.725 8.725 5 9.615 5h9.77C20.275 5 21 5.725 21 6.615V23z" fill-rule="evenodd"/></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" ><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4z" fill-rule="evenodd"/></svg></span></span></button></li><li class="u-marginVertical10 u-marginLeft3"><a class="button button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon button--dark button--chromeless"   href="https://medium.com/p/3888677c5f4d/share/twitter" title="Share on Twitter" aria-label="Share on Twitter" target="_blank" data-action-source="post_share_widget"><span class="button-defaultState"><span class="svgIcon svgIcon--twitterFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" ><path d="M22.053 7.54a4.474 4.474 0 0 0-3.31-1.455 4.526 4.526 0 0 0-4.526 4.524c0 .35.04.7.082 1.05a12.9 12.9 0 0 1-9.3-4.77c-.39.69-.61 1.46-.65 2.26.03 1.6.83 2.99 2.02 3.79-.72-.02-1.41-.22-2.02-.57-.01.02-.01.04 0 .08-.01 2.17 1.55 4 3.63 4.44-.39.08-.79.13-1.21.16-.28-.03-.57-.05-.81-.08.54 1.77 2.21 3.08 4.2 3.15a9.564 9.564 0 0 1-5.66 1.94c-.34-.03-.7-.06-1.05-.08 2 1.27 4.38 2.02 6.94 2.02 8.31 0 12.86-6.9 12.84-12.85.02-.24.01-.43 0-.65.89-.62 1.65-1.42 2.26-2.34-.82.38-1.69.62-2.59.72a4.37 4.37 0 0 0 1.94-2.51c-.84.53-1.81.9-2.83 1.13z"/></svg></span></span></a></li><li class="u-marginVertical10 u-marginLeft3"><a class="button button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon button--dark button--chromeless"   href="https://medium.com/p/3888677c5f4d/share/facebook" title="Share on Facebook" aria-label="Share on Facebook" target="_blank" data-action-source="post_share_widget"><span class="button-defaultState"><span class="svgIcon svgIcon--facebookSquare svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" ><path d="M23.209 5H5.792A.792.792 0 0 0 5 5.791V23.21c0 .437.354.791.792.791h9.303v-7.125H12.72v-2.968h2.375v-2.375c0-2.455 1.553-3.662 3.741-3.662 1.049 0 1.95.078 2.213.112v2.565h-1.517c-1.192 0-1.469.567-1.469 1.397v1.963h2.969l-.594 2.968h-2.375L18.11 24h5.099a.791.791 0 0 0 .791-.791V5.79a.791.791 0 0 0-.791-.79"/></svg></span></span></a></li></ul></div></aside><div class="u-fixed u-bottom0 u-width100pct u-backgroundWhite u-boxShadowTop u-borderBox u-paddingTop10 u-paddingBottom10 u-zIndexMetabar u-xs-hide js-stickyFooter"><div class="u-maxWidth700 u-marginAuto u-flexCenter"><div class="u-fontSize16 u-flex1 u-flexCenter"><div class="u-flex0 u-inlineBlock u-paddingRight20"><a class="link u-baseColor--link avatar u-inline"   href="https://medium.com/@jonathan_hui" data-action="show-user-card" data-action-value="bd51f1a63813" data-action-type="hover" data-user-id="bd51f1a63813" dir="auto"><img  src="https://cdn-images-1.medium.com/fit/c/80/80/1*c3Z3aOPBooxEX4tx4RkzLw.jpeg" class="avatar-image avatar-image--smaller" alt="Go to the profile of Jonathan Hui"></a></div><div class="u-flex1 u-inlineBlock">Never miss a story from<strong> Jonathan Hui</strong>, when you sign up for Medium. <a class="link u-baseColor--link link--accent u-accentColor--textNormal u-accentColor--textDarken"   href="https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg" data-action-source="sticky_footer">Learn more</a></div></div><div class="u-marginLeft50"><span class="followState js-followState" data-user-id="bd51f1a63813"><button class="button u-noUserSelect button--withChrome u-baseColor--buttonNormal button--withHover button--unblock js-unblockButton u-uiTextSemibold u-textUppercase u-fontSize12"  data-action="sign-up-prompt" data-sign-in-action="toggle-block-user" data-requires-token="true" data-redirect="https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d" data-action-source="sticky_footer"><span class="button-label  button-defaultState">Blocked</span><span class="button-label button-hoverState">Unblock</span></button><button class="button button--primary is-active u-noUserSelect button--withChrome u-accentColor--buttonNormal button--follow js-followButton u-uiTextSemibold u-textUppercase u-fontSize12"  data-action="sign-up-prompt" data-sign-in-action="toggle-subscribe-user" data-requires-token="true" data-redirect="https://medium.com/_/subscribe/user/bd51f1a63813" data-action-source="sticky_footer-bd51f1a63813-------------------------follow_metabar"><span class="button-label  button-defaultState js-buttonLabel">Follow</span><span class="button-label button-activeState">Get updates</span></button></span></div></div></div></div></div></div><div class="loadingBar"></div><script>// <![CDATA[
window["obvInit"] = function (opt_embedded) {window["obvInit"]["embedded"] = opt_embedded; window["obvInit"]["ready"] = true;}
// ]]></script><script>// <![CDATA[
var GLOBALS = {"audioUrl":"https://d1fcbxp97j4nb2.cloudfront.net","baseUrl":"https://medium.com","buildLabel":"37640-998bb54","currentUser":{"userId":"lo_xxEwLfhil7sg","isVerified":false,"subscriberEmail":"","hasPastMemberships":false,"isEnrolledInHightower":false,"isEligibleForHightower":false,"hightowerLastLockedAt":0,"isWriterProgramEnrolled":true,"isWriterProgramInvited":false,"isWriterProgramOptedOut":false,"writerProgramVersion":0,"writerProgramEnrolledAt":0,"friendLinkOnboarding":0,"hasAdditionalUnlocks":false,"hasApiAccess":false,"isQuarantined":false,"writerProgramDistributionSettingOptedIn":false},"currentUserHasUnverifiedEmail":false,"isAuthenticated":false,"isCurrentUserVerified":false,"language":"en-au","miroUrl":"https://cdn-images-1.medium.com","moduleUrls":{"base":"https://cdn-static-1.medium.com/_/fp/gen-js/main-base.bundle.6a1tBZuDquIeSAu9-NeYtQ.js","common-async":"https://cdn-static-1.medium.com/_/fp/gen-js/main-common-async.bundle.EAz5MoTAys8fxpp7U4hETg.js","hightower":"https://cdn-static-1.medium.com/_/fp/gen-js/main-hightower.bundle.f4naSX5lPGrCJudIQqUFkw.js","home-screens":"https://cdn-static-1.medium.com/_/fp/gen-js/main-home-screens.bundle.8dmOg2owNER0j0yLKFFOZw.js","misc-screens":"https://cdn-static-1.medium.com/_/fp/gen-js/main-misc-screens.bundle.bMsHQn2JPJriABI7HeQJQg.js","notes":"https://cdn-static-1.medium.com/_/fp/gen-js/main-notes.bundle.r17vuw57gl0PSNF7O0Cx9A.js","payments":"https://cdn-static-1.medium.com/_/fp/gen-js/main-payments.bundle._GhGmWAjbt6cl33MjaZOBg.js","posters":"https://cdn-static-1.medium.com/_/fp/gen-js/main-posters.bundle.kOmQiIwjpU2fhmMALBsIjw.js","power-readers":"https://cdn-static-1.medium.com/_/fp/gen-js/main-power-readers.bundle.JirImWO9lfQV83pAAauVLg.js","pubs":"https://cdn-static-1.medium.com/_/fp/gen-js/main-pubs.bundle.HMmyKDIK_MUfN3gHSs7upA.js","stats":"https://cdn-static-1.medium.com/_/fp/gen-js/main-stats.bundle.kQ5W7UAYsS5jRLUYgrI0sw.js"},"previewConfig":{"weightThreshold":1,"weightImageParagraph":0.51,"weightIframeParagraph":0.8,"weightTextParagraph":0.08,"weightEmptyParagraph":0,"weightP":0.003,"weightH":0.005,"weightBq":0.003,"minPTextLength":60,"truncateBoundaryChars":20,"detectTitle":true,"detectTitleLevThreshold":0.15},"productName":"Medium","supportsEdit":true,"termsUrl":"//medium.com/policy/9db0094a1e0f","textshotHost":"textshot.medium.com","transactionId":"1559043093038:2fdd8447736b","useragent":{"browser":"safari","family":"safari","os":"mac","version":12.1,"supportsDesktopEdit":true,"supportsInteract":true,"supportsView":true,"isMobile":false,"isTablet":false,"isNative":false,"supportsFileAPI":true,"isTier1":true,"clientVersion":"","unknownParagraphsBad":false,"clientChannel":"","supportsRealScrollEvents":true,"supportsVhUnits":true,"ruinsViewportSections":false,"supportsHtml5Video":true,"supportsMagicUnderlines":true,"isWebView":false,"isFacebookWebView":false,"supportsProgressiveMedia":true,"supportsPromotedPosts":true,"isBot":false,"isNativeIphone":false,"supportsCssVariables":true,"supportsVideoSections":true,"emojiSupportLevel":5,"isSearchBot":false,"isSyndicationBot":false,"isNativeAndroid":false,"isNativeIos":false,"supportsScrollableMetabar":true},"variants":{"allow_access":true,"allow_signup":true,"allow_test_auth":"disallow","signin_services":"twitter,facebook,google,email,google-fastidv,google-one-tap","signup_services":"twitter,facebook,google,email,google-fastidv,google-one-tap","google_sign_in_android":true,"reengagement_notification_duration":3,"browsable_stream_config_bucket":"curated-topics","enable_dedicated_series_tab_api_ios":true,"enable_post_import":true,"available_monthly_plan":"60e220181034","available_annual_plan":"2c754bcc2995","disable_ios_resume_reading_toast":true,"is_not_medium_subscriber":true,"glyph_font_set":"m2","enable_branding":true,"enable_branding_fonts":true,"max_premium_content_per_user_under_metering":3,"enable_automated_mission_control_triggers":true,"enable_lite_profile":true,"enable_marketing_emails":true,"enable_topic_lifecycle_email":true,"enable_parsely":true,"enable_branch_io":true,"enable_ios_post_stats":true,"enable_lite_topics":true,"enable_lite_stories":true,"redis_read_write_splitting":true,"enable_tipalti_onboarding":true,"enable_international_tax_withholding":true,"enable_international_tax_withholding_documentation":true,"enable_revised_first_partner_program_distro_on_email":true,"enable_annual_renewal_reminder_email":true,"enable_janky_spam_rules":"users,posts","enable_new_collaborative_filtering_data":true,"android_rating_prompt_stories_read_threshold":2,"stripe_v3":true,"enable_google_one_tap":true,"enable_email_sign_in_captcha":true,"enable_primary_topic_for_mobile":true,"enable_todays_highlights_android":true,"enable_logged_out_homepage_signup":true,"use_new_admin_topic_backend":true,"enable_quarantine_rules":true,"enable_patronus_on_kubernetes":true,"pub_sidebar":true,"disable_mobile_featured_chunk":true,"enable_pub_newsletters":true,"enable_new_user_avatar_dropdown_menu":true,"enable_mobile_pubcrawl_home_feed":true,"enable_send_pub_digest":true,"enable_draft_in_post_cotent":true,"enable_paypal_cancel_webhook":true,"enable_retrained_ranking_model_digest":true,"enable_retrained_ranking_model_homepage":true,"disable_ensure_featured_distro_processor":true},"xsrfToken":"","iosAppId":"828256236","supportEmail":"yourfriends@medium.com","fp":{"/icons/monogram-mask.svg":"https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg","/icons/favicon-dev-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-dev-editor.YKKRxBO8EMvIqhyCwIiJeQ.ico","/icons/favicon-hatch-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-hatch-editor.BuEyHIqlyh2s_XEk4Rl32Q.ico","/icons/favicon-medium-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-medium-editor.PiakrZWB7Yb80quUVQWM6g.ico"},"authBaseUrl":"https://medium.com","imageUploadSizeMb":25,"isAuthDomainRequest":true,"algoliaApiEndpoint":"https://MQ57UUUQZ2-dsn.algolia.net","algoliaAppId":"MQ57UUUQZ2","algoliaSearchOnlyApiKey":"394474ced050e3911ae2249ecc774921","iosAppStoreUrl":"https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8","iosAppLinkBaseUrl":"medium:","algoliaIndexPrefix":"medium_","androidPlayStoreUrl":"https://play.google.com/store/apps/details?id=com.medium.reader","googleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","androidPackage":"com.medium.reader","androidPlayStoreMarketScheme":"market://details?id=com.medium.reader","googleAuthUri":"https://accounts.google.com/o/oauth2/auth","androidScheme":"medium","layoutData":{"useDynamicScripts":false,"googleAnalyticsTrackingCode":"UA-24232453-2","jsShivUrl":"https://cdn-static-1.medium.com/_/fp/js/shiv.RI2ePTZ5gFmMgLzG5bEVAA.js","useDynamicCss":false,"faviconUrl":"https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium.3Y6xpZ-0FSdWDnPM3hSBIA.ico","faviconImageId":"1*8I-HPL0bfoIzGied-dzOvA.png","fontSets":[{"id":8,"url":"https://glyph.medium.com/css/e/sr/latin/e/ssr/latin/e/ssb/latin/m2.css"},{"id":11,"url":"https://glyph.medium.com/css/m2.css"},{"id":9,"url":"https://glyph.medium.com/css/mkt.css"}],"editorFaviconUrl":"https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium-editor.3Y6xpZ-0FSdWDnPM3hSBIA.ico","glyphUrl":"https://glyph.medium.com"},"authBaseUrlRev":"moc.muidem//:sptth","isDnt":false,"stripePublishableKey":"pk_live_7FReX44VnNIInZwrIIx6ghjl","archiveUploadSizeMb":100,"paymentData":{"currencies":{"1":{"label":"US Dollar","external":"usd"}},"countries":{"1":{"label":"United States of America","external":"US"}},"accountTypes":{"1":{"label":"Individual","external":"individual"},"2":{"label":"Company","external":"company"}}},"previewConfig2":{"weightThreshold":1,"weightImageParagraph":0.05,"raiseImage":true,"enforceHeaderHierarchy":true,"isImageInsetRight":true},"isAmp":false,"iosScheme":"medium","isSwBoot":false,"lightstep":{"accessToken":"ce5be895bef60919541332990ac9fef2","carrier":"{\"ot-tracer-spanid\":\"1633b6d22a8b4fee\",\"ot-tracer-traceid\":\"78ec37fb1ae17228\",\"ot-tracer-sampled\":\"true\"}","host":"collector-medium.lightstep.com"},"facebook":{"key":"542599432471018","namespace":"medium-com","scope":{"default":["public_profile","email"],"connect":["public_profile","email"],"login":["public_profile","email"],"share":["public_profile","email"]}},"editorsPicksTopicId":"3985d2a191c5","popularOnMediumTopicId":"9d34e48ecf94","memberContentTopicId":"13d7efd82fb2","audioContentTopicId":"3792abbd134","brandedSequenceId":"7d337ddf1941","isDoNotAuth":false,"buggle":{"url":"https://buggle.medium.com","videoUrl":"https://cdn-videos-1.medium.com","audioUrl":"https://cdn-audio-1.medium.com"},"referrerType":2,"isMeteredOut":false,"meterConfig":{"maxUnlockCount":3,"windowLength":"MONTHLY"},"partnerProgramEmail":"partnerprogram@medium.com","userResearchPrompts":[{"promptId":"lo_post_page_4","type":0,"url":"www.calendly.com"},{"promptId":"lo_home_page","type":1,"url":"www.calendly.com"},{"promptId":"lo_profile_page","type":2,"url":"www.calendly.com"}],"recaptchaKey":"6LdAokEUAAAAAC7seICd4vtC8chDb3jIXDQulyUJ","signinWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"countryCode":"AU","bypassMeter":false,"branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","paypal":{"clientMode":"production","oneYearGift":{"name":"Medium Membership (1 Year, Digital Gift Code)","description":"Unlimited access to the best and brightest stories on Medium. Gift codes can be redeemed at medium.com/redeem.","price":"50.00","currency":"USD","sku":"membership-gift-1-yr"}},"collectionConfig":{"mediumOwnedAndOperatedCollectionIds":["544c7006046e","bcc38c8f6edf","444d13b52878","8d6b8a439e32","92d2092dc598","1285ba81cada","cb8577c9149e"]}}
// ]]></script><script charset="UTF-8" src="https://cdn-static-1.medium.com/_/fp/gen-js/main-base.bundle.6a1tBZuDquIeSAu9-NeYtQ.js" async></script><script>// <![CDATA[
window["obvInit"]({"value":{"id":"3888677c5f4d","versionId":"8b17333d970c","creatorId":"bd51f1a63813","creator":{"userId":"bd51f1a63813","name":"Jonathan Hui","username":"jonathan_hui","createdAt":1518417490123,"imageId":"1*c3Z3aOPBooxEX4tx4RkzLw.jpeg","backgroundImageId":"","bio":"Deep Learning","twitterScreenName":"","socialStats":{"userId":"bd51f1a63813","usersFollowedCount":16,"usersFollowedByCount":8172,"type":"SocialStats"},"social":{"userId":"lo_xxEwLfhil7sg","targetUserId":"bd51f1a63813","type":"Social"},"facebookAccountId":"","allowNotes":1,"mediumMemberAt":0,"isNsfw":false,"isWriterProgramEnrolled":true,"isQuarantined":false,"type":"User"},"homeCollectionId":"","title":"What do we learn from single shot object detectors (SSD, YOLOv3), FPN & Focal loss (RetinaNet)?","detectedLanguage":"en","latestVersion":"8b17333d970c","latestPublishedVersion":"8b17333d970c","hasUnpublishedEdits":false,"latestRev":523,"createdAt":1522209795051,"updatedAt":1555349409430,"acceptedAt":0,"firstPublishedAt":1522217617724,"latestPublishedAt":1555349409430,"vote":false,"experimentalCss":"","displayAuthor":"","content":{"subtitle":"In part 2, we will have a comprehensive review of single shot object detectors including SSD and YOLO (YOLOv2 and YOLOv3). We will also…","bodyModel":{"paragraphs":[{"name":"b699","type":4,"text":"","markups":[],"layout":3,"metadata":{"id":"0*OG_cQNTRdw0CYGyU.","originalWidth":3000,"originalHeight":1735,"isFeatured":true}},{"name":"4928","type":3,"text":"What do we learn from single shot object detectors (SSD, YOLOv3), FPN & Focal loss (RetinaNet)?","markups":[]},{"name":"b8c0","type":1,"text":"In part 2, we will have a comprehensive review of single shot object detectors including SSD and YOLO (YOLOv2 and YOLOv3). We will also look into FPN to see how a pyramid of multi-scale feature maps will improve accuracy, in particular for small objects that usually perform badly for single shot detectors. Then we will look into Focal loss and RetinaNet on how it solve class imbalance problem during training.","markups":[]},{"name":"5461","type":1,"text":"Part 1: What do we learn from region based object detectors (Faster R-CNN, R-FCN, FPN)?","markups":[{"type":3,"start":0,"end":6,"href":"https://medium.com/@jonathan_hui/what-do-we-learn-from-region-based-object-detectors-faster-r-cnn-r-fcn-fpn-7e354377a7c9","title":"","rel":"","anchorType":0}]},{"name":"b144","type":1,"text":"Part 2: What do we learn from single shot object detectors (SSD, YOLO), FPN & Focal loss?","markups":[{"type":3,"start":0,"end":6,"href":"https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d","title":"","rel":"","anchorType":0}]},{"name":"f030","type":1,"text":"Part 3: Design choices, lessons learned and trends for object detections?","markups":[{"type":3,"start":0,"end":6,"href":"https://medium.com/@jonathan_hui/design-choices-lessons-learned-and-trends-for-object-detections-4f48b59ec5ff","title":"","rel":"","anchorType":0}]},{"name":"5871","type":3,"text":"Single Shot detectors","markups":[]},{"name":"55e6","type":1,"text":"Faster R-CNN has a dedicated region proposal network followed by a classifier.","markups":[]},{"name":"1de8","type":4,"text":"Faster R-CNN flow","markups":[],"layout":1,"metadata":{"id":"1*F-WbcUMpWSE1tdKRgew2Ug.png","originalWidth":1455,"originalHeight":447}},{"name":"43b5","type":1,"text":"Region-based detectors are accurate but not without a cost. Faster R-CNN processes about 7 FPS (frame per second) for PASCAL VOC 2007 testing set. Like R-FCN, researchers are streamlining the process by reducing the amount of work for each ROI.","markups":[{"type":1,"start":91,"end":94}]},{"name":"1627","type":8,"text":"feature_maps = process(image)\nROIs = region_proposal(feature_maps)\nfor ROI in ROIs\n    patch = roi_align(feature_maps, ROI)\n    results = detector2(patch)    # Reduce the amount of work here!","markups":[]},{"name":"a84d","type":1,"text":"As an alternative, do we need a separate region proposal step? Can we derive both boundary boxes and classes directly from feature maps in one step?","markups":[]},{"name":"a55f","type":8,"text":"feature_maps = process(image)\nresults = detector3(feature_maps) # No more separate step for ROIs","markups":[]},{"name":"7cfb","type":1,"text":"Let’s look at the sliding-window detector again. We can slide windows over feature maps to detect objects. For different object types, we use different window shapes. The fatal mistake of the previous sliding-windows is that we use the windows as the final boundary boxes. For that, we need too many shapes to cover most objects. A more effective solution is to treat the window as an initial guess. Then we have a detector to predict the class and the boundary box from the current sliding window simultaneously.","markups":[]},{"name":"ccf2","type":4,"text":"Making a prediction relative to a sliding window.","markups":[],"layout":3,"metadata":{"id":"1*tE6DUwv6VIHu1KlwYmSBTw.jpeg","originalWidth":1123,"originalHeight":561}},{"name":"e5c3","type":1,"text":"This concept is very similar to the anchors in Faster R-CNN. However, single shot detectors predict both the boundary box and the class at the same time. Let’s do a quick recap. For example, we have an 8 × 8 feature map and we make k predictions at each location. i.e. 8 × 8 × k predictions.","markups":[{"type":1,"start":232,"end":233}]},{"name":"8b81","type":4,"text":"64 locations","markups":[],"layout":1,"metadata":{"id":"1*i2egSyxtuJo3YYjdLbaBGQ.png","originalWidth":336,"originalHeight":375}},{"name":"75ad","type":1,"text":"At each location, we have k anchors (anchors are just fixed initial boundary box guesses), one anchor for one specific prediction. We select the anchors carefully and every location uses the same anchor shapes.","markups":[]},{"name":"91f9","type":4,"text":"Use 4 anchors to make 4 predictions per location.","markups":[],"layout":1,"metadata":{"id":"1*1F8rWQyBV-P8pDn0Avx-OA.png","originalWidth":335,"originalHeight":337}},{"name":"8e35","type":1,"text":"Here are 4 anchors (green) and 4 corresponding predictions (blue) each related to one specific anchor.","markups":[]},{"name":"050d","type":4,"text":"4 predictions each relative to an anchor","markups":[],"layout":1,"metadata":{"id":"1*TjZ-YFE1YLPNOJJzjyFCEQ.jpeg","originalWidth":1123,"originalHeight":561}},{"name":"c5c8","type":1,"text":"In Faster R-CNN, we use one convolution filter to make a 5-parameter prediction: 4 parameters on the predicted box relative to an anchor and 1 objectness confidence score. So the 3× 3× D × 5 convolution filter transforms the feature maps from 8 × 8 × D to 8 × 8 × 5.","markups":[]},{"name":"60cc","type":4,"text":"Compute a prediction using a 3x3 convolution filter.","markups":[],"layout":1,"metadata":{"id":"1*yrAA9xnL4OlhX6RoeHQVtQ.png","originalWidth":708,"originalHeight":382}},{"name":"1ec5","type":1,"text":"In a single shot detector, the convolution filter also predicts C class probabilities for classification (one per class). So we apply a 3× 3× D × 25 convolution filter to transform the feature maps from 8 × 8 × D to 8 × 8 × 25 for C=20.","markups":[{"type":1,"start":64,"end":65}]},{"name":"eea5","type":4,"text":"Each location makes k predictions each have 25 parameters.","markups":[],"layout":1,"metadata":{"id":"1*UsqjfoW3sLkmyXKQ0Hyo8A.png","originalWidth":1047,"originalHeight":476}},{"name":"e998","type":1,"text":"Single shot detector often trades accuracy with real-time processing speed. They also tend to have issues in detecting objects that are too close or too small. For the picture below, there are 9 Santas in the lower left corner but one of the single shot detectors detects 5 only.","markups":[]},{"name":"1455","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*j4PnWfxP3yoVPOFyI27tww.jpeg","originalWidth":3036,"originalHeight":1855}},{"name":"c48f","type":3,"text":"SSD","markups":[]},{"name":"e8fb","type":1,"text":"SSD is a single shot detector using a VGG16 network as a feature extractor (equivalent to the CNN in Faster R-CNN). Then we add custom convolution layers (blue) afterward and use convolution filters (green) to make predictions.","markups":[]},{"name":"5902","type":4,"text":"Single shot prediction for both classification and location.","markups":[],"layout":3,"metadata":{"id":"1*1C5hgYTdBvCdCYWbXEaVww.png","originalWidth":2182,"originalHeight":196}},{"name":"73de","type":1,"text":"However, convolution layers reduce spatial dimension and resolution. So the model above can detect large objects only. To fix that, we make independent object detections from multiple feature maps.","markups":[]},{"name":"0de0","type":4,"text":"Use multi-scale feature maps for detection.","markups":[],"layout":3,"metadata":{"id":"1*k0eFZw1jlF9xPvhzBKt6LQ.png","originalWidth":1419,"originalHeight":310}},{"name":"db27","type":1,"text":"Here is the diagram showing the dimensions of feature maps.","markups":[]},{"name":"8103","type":4,"text":"Source","markups":[{"type":3,"start":0,"end":6,"href":"https://arxiv.org/pdf/1512.02325.pdf","title":"","rel":"noopener","anchorType":0}],"layout":3,"metadata":{"id":"1*up-gIJ9rPkHXUGRoqWuULQ.jpeg","originalWidth":1435,"originalHeight":483}},{"name":"66ab","type":1,"text":"SSD uses layers already deep down into the convolutional network to detect objects. If we redraw the diagram closer to scale, we should realize the spatial resolution has dropped significantly and may already miss the opportunity in locating small objects that are too hard to detect in low resolution. If such problem exists, we need to increase the resolution of the input image.","markups":[]},{"name":"dc75","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*oCaj3OQIbhOGSxcgvONTQw.png","originalWidth":1626,"originalHeight":1048}},{"name":"3c1e","type":1,"text":"YOLO","markups":[{"type":1,"start":0,"end":4}]},{"name":"8f09","type":1,"text":"YOLO is another single shot detector.","markups":[]},{"name":"da74","type":11,"text":"Object detection in real-time","markups":[],"layout":1,"iframe":{"mediaResourceId":"b901faec3876bb5da6091d9410fd9c72","iframeWidth":854,"iframeHeight":480,"thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Fi.ytimg.com%2Fvi%2FVOC3huqHrss%2Fhqdefault.jpg&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"e733","type":1,"text":"YOLO uses DarkNet to make feature detection followed by convolutional layers.","markups":[]},{"name":"8ac0","type":4,"text":"","markups":[],"layout":3,"metadata":{"id":"1*NBnDpz8fitkhcdnkgF2bvg.png","originalWidth":1794,"originalHeight":211}},{"name":"f5ff","type":1,"text":"However, it does not make independent detections using multi-scale feature maps. Instead, it partially flattens features maps and concatenates it with another lower resolution maps. For example, YOLO reshapes a 28 × 28 × 512 layer to 14 × 14 × 2048. Then it concatenates with the 14 × 14 ×1024 feature maps. Afterward, YOLO applies convolution filters on the new 14 × 14 × 3072 layer to make predictions.","markups":[]},{"name":"a4f2","type":1,"text":"YOLO (v2) makes many implementation improvements to push the mAP from 63.4 for the first release to 78.6. YOLO9000 can detect 9000 different categories of objects.","markups":[]},{"name":"e4b5","type":4,"text":"Source","markups":[{"type":3,"start":0,"end":6,"href":"https://arxiv.org/pdf/1612.08242.pdf","title":"","rel":"noopener","anchorType":0}],"layout":1,"metadata":{"id":"1*3IdCKSzR5R0lIE1LSmN4Bg.png","originalWidth":1568,"originalHeight":648}},{"name":"ba43","type":1,"text":"Here are the mAP and FPS comparison for different detectors reported by the YOLO paper. YOLOv2 can take different input image resolutions. Lower resolution input images achieve higher FPS but lower mAP.","markups":[]},{"name":"4e7e","type":4,"text":"Source","markups":[{"type":3,"start":0,"end":6,"href":"https://arxiv.org/pdf/1612.08242.pdf","title":"","rel":"noopener","anchorType":0}],"layout":1,"metadata":{"id":"1*NJj17Z6FgffYaA4WH2WIjw.png","originalWidth":462,"originalHeight":325}},{"name":"f271","type":1,"text":"YOLOv3","markups":[{"type":1,"start":0,"end":6}]},{"name":"d3ee","type":1,"text":"YOLOv3 change to a more complex backbone for feature extraction. Darknet-53 mainly compose of 3 × 3 and 1× 1 filters with skip connections like the residual network in ResNet. Darknet-53 has less BFLOP (billion floating point operations) than ResNet-152, but achieves the same classification accuracy at 2x faster.","markups":[]},{"name":"f720","type":4,"text":"Source","markups":[{"type":3,"start":0,"end":6,"href":"https://pjreddie.com/media/files/papers/YOLOv3.pdf","title":"","rel":"","anchorType":0}],"layout":1,"metadata":{"id":"1*biRYJyCSv-UTbTQTa4Afqg.png","originalWidth":334,"originalHeight":462}},{"name":"d540","type":1,"text":"YOLOv3 also adds Feature Pyramid (discussed next) to detect small objects better. Here is the accuracy and speed tradeoff for different detectors.","markups":[]},{"name":"81d1","type":4,"text":"Source","markups":[{"type":3,"start":0,"end":6,"href":"https://pjreddie.com/media/files/papers/YOLOv3.pdf","title":"","rel":"noopener","anchorType":0}],"layout":1,"metadata":{"id":"1*rfj_5yjKZm2LJvVzMXmLFA.png","originalWidth":800,"originalHeight":498}},{"name":"bcfc","type":3,"text":"Feature Pyramid Networks (FPN)","markups":[]},{"name":"b4fb","type":1,"text":"Detecting objects in different scales is challenging in particular for small objects. Feature Pyramid Network (FPN) is a feature extractor designed with feature pyramid concept to improve accuracy and speed. It replaces the feature extractor of detectors like Faster R-CNN and generates higher quality feature map pyramid.","markups":[{"type":1,"start":111,"end":114}]},{"name":"5a42","type":1,"text":"Data Flow","markups":[{"type":1,"start":0,"end":9}]},{"name":"5f14","type":4,"text":"FPN (Source)","markups":[{"type":3,"start":5,"end":11,"href":"https://arxiv.org/pdf/1612.03144.pdf","title":"","rel":"noopener","anchorType":0}],"layout":1,"metadata":{"id":"1*aMRoAN7CtD1gdzTaZIT5gA.png","originalWidth":500,"originalHeight":201}},{"name":"c11c","type":1,"text":"FPN composes of a bottom-up and a top-down pathway. The bottom-up pathway is the usual convolutional network for feature extraction. As we go up, the spatial resolution decreases. With more high-level structures detected, the semantic value for each layer increases.","markups":[{"type":1,"start":18,"end":27},{"type":1,"start":34,"end":42},{"type":1,"start":226,"end":240}]},{"name":"9030","type":4,"text":"Feature extraction in FPN (Modified from source)","markups":[{"type":3,"start":41,"end":47,"href":"https://arxiv.org/pdf/1612.03144.pdf","title":"","rel":"noopener","anchorType":0}],"layout":1,"metadata":{"id":"1*_kxgFskpRJ6bsxEjh9CH6g.jpeg","originalWidth":470,"originalHeight":318}},{"name":"ff43","type":1,"text":"SSD makes detection from multiple feature maps. However, the bottom layers are not selected for object detection. They are in high resolution but the semantic value is not high enough to justify its use as the speed slow-down is significant. So SSD only uses upper layers for detection and therefore performs much worse for small objects.","markups":[]},{"name":"3038","type":4,"text":"Modified from source","markups":[{"type":3,"start":14,"end":20,"href":"https://arxiv.org/pdf/1612.03144.pdf","title":"","rel":"noopener","anchorType":0}],"layout":1,"metadata":{"id":"1*M_c6Jx5Uy7qr6vJbrtAvhg.png","originalWidth":400,"originalHeight":246}},{"name":"de21","type":1,"text":"FPN provides a top-down pathway to construct higher resolution layers from a semantic rich layer.","markups":[]},{"name":"f1f7","type":4,"text":"Reconstruct spatial resolution in the top-down pathway. (Modified from source)","markups":[{"type":3,"start":71,"end":77,"href":"https://arxiv.org/pdf/1612.03144.pdf","title":"","rel":"noopener","anchorType":0}],"layout":1,"metadata":{"id":"1*XmNDHT8WWZbXACyBjg3ZeQ.jpeg","originalWidth":500,"originalHeight":201}},{"name":"fc73","type":1,"text":"While the reconstructed layers are semantic strong but the locations of objects are not precise after all the downsampling and upsampling. We add lateral connections between reconstructed layers and the corresponding feature maps to help the detector to predict the location betters.","markups":[]},{"name":"3fb5","type":4,"text":"Add skip connections (Source)","markups":[{"type":3,"start":22,"end":28,"href":"https://arxiv.org/pdf/1612.03144.pdf","title":"","rel":"noopener","anchorType":0}],"layout":1,"metadata":{"id":"1*aMRoAN7CtD1gdzTaZIT5gA.png","originalWidth":500,"originalHeight":201}},{"name":"8a0e","type":1,"text":"The following is a detail diagram on the bottom-up and the top-down pathway. P2, P3, P4 and P5 are the pyramid of feature maps for object detection.","markups":[]},{"name":"0ae5","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*ffxP_rL8-jMvipLhMJrVeA.png","originalWidth":968,"originalHeight":1134}},{"name":"9592","type":1,"text":"FPN with RPN","markups":[{"type":1,"start":0,"end":12}]},{"name":"4d45","type":1,"text":"FPN is not an object detector by itself. It is a feature detector that works with object detectors. The following feed each feature maps (P2 to P5) independently to make object detection.","markups":[]},{"name":"7301","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*cHR4YRqdPBOx4IDqzU-GwQ.png","originalWidth":1602,"originalHeight":1138}},{"name":"5810","type":1,"text":"FPN with Fast R-CNN or Faster R-CNN","markups":[{"type":1,"start":0,"end":35}]},{"name":"635d","type":1,"text":"In FPN, we generate a pyramid of feature maps. We apply the RPN (described above) to generate ROIs. Based on the size of the ROI, we select the feature map layer in the most proper scale to extract the feature patches.","markups":[]},{"name":"775e","type":4,"text":"","markups":[],"layout":3,"metadata":{"id":"1*Wvn0WG4XZ0w9Ed2fFYPrXw.jpeg","originalWidth":1000,"originalHeight":368}},{"name":"ccd4","type":3,"text":"Hard example mining","markups":[{"type":1,"start":0,"end":19}]},{"name":"d34b","type":1,"text":"For most detectors like SSD and YOLO, we make far more predictions than the number of objects presence. So there are much more negative matches than positive matches. This creates a class imbalance which hurts training. We are training the model to learn background space rather than detecting objects. However, we need negative sampling so it can learn what constitutes a bad prediction. So, for example in SSD, we sort training examples by their calculated confidence loss. We pick the top ones and makes sure the ratio between the picked negatives and positives is at most 3:1. This leads to a faster and more stable training.","markups":[]},{"name":"5c31","type":3,"text":"Non-maximal suppression in inference","markups":[{"type":1,"start":0,"end":36}]},{"name":"9c17","type":1,"text":"Detectors can make duplicate detections for the same object. To fix this, we apply non-maximal suppression to remove duplications with lower confidence. We sort the predictions by the confidence scores and go down the list one by one. If any previous prediction has the same class and IoU greater than 0.5 with the current prediction, we remove it from the list.","markups":[]},{"name":"0f20","type":3,"text":"Focal loss (RetinaNet)","markups":[]},{"name":"970c","type":1,"text":"Class imbalance hurts performance. SSD resamples the ratio of the object class and background class during training so it will not be overwhelmed by image background. Focal loss (FL) adopts another approach to reduce loss for well-trained class. So whenever the model is good at detecting background, it will reduce its loss and reemphasize the training on the object class. We start with the cross-entropy loss CE and we add a weight to de-emphasize the CE for high confidence class.","markups":[]},{"name":"e8db","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*DgI0JPk98eXfLvmkK_9PGQ.png","originalWidth":1486,"originalHeight":378}},{"name":"0305","type":1,"text":"For example, for γ = 0.5, the focal loss for well-classified examples will be pushed toward 0.","markups":[]},{"name":"db52","type":4,"text":"Modified from source.","markups":[{"type":3,"start":14,"end":20,"href":"https://arxiv.org/pdf/1708.02002.pdf","title":"","rel":"noopener","anchorType":0}],"layout":1,"metadata":{"id":"1*FCV96tP679EScoiwKq4IaQ.png","originalWidth":400,"originalHeight":240}},{"name":"4717","type":1,"text":"Here is the RetinaNet building on FPN and ResNet using the Focal loss.","markups":[]},{"name":"4da3","type":4,"text":"RetinaNet","markups":[{"type":3,"start":0,"end":9,"href":"https://arxiv.org/pdf/1708.02002.pdf","title":"","rel":"noopener","anchorType":0}],"layout":3,"metadata":{"id":"1*jQFeF7gj6uCXVzUb08S9lg.png","originalWidth":1906,"originalHeight":510}},{"name":"3813","type":3,"text":"Further reading on SSD, YOLO & FPN","markups":[]},{"name":"dcf5","type":1,"text":"Both SSD and YOLO are more complex than we described here. For further study, please refer to:","markups":[]},{"name":"74ba","type":9,"text":"SSD object detection.","markups":[{"type":3,"start":0,"end":20,"href":"https://medium.com/@jonathan_hui/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06","title":"","rel":"","anchorType":0}]},{"name":"2669","type":9,"text":"YOLO object detection.","markups":[{"type":3,"start":0,"end":21,"href":"https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088","title":"","rel":"","anchorType":0}]},{"name":"21cb","type":9,"text":"FPN object detection.","markups":[{"type":3,"start":0,"end":20,"href":"https://medium.com/@jonathan_hui/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c","title":"","rel":"","anchorType":0}]},{"name":"528e","type":3,"text":"Resources","markups":[]},{"name":"7e93","type":1,"text":"Please refer to the SSD and YOLO article for the corresponding implementations.","markups":[{"type":3,"start":20,"end":23,"href":"https://medium.com/@jonathan_hui/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06","title":"","rel":"","anchorType":0},{"type":3,"start":28,"end":32,"href":"https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088","title":"","rel":"","anchorType":0}]}],"sections":[{"name":"b530","startIndex":0}]},"postDisplay":{"coverless":true}},"virtuals":{"allowNotes":true,"previewImage":{"imageId":"0*OG_cQNTRdw0CYGyU.","filter":"","backgroundSize":"","originalWidth":3000,"originalHeight":1735,"strategy":"resample","height":0,"width":0},"wordCount":1633,"imageCount":29,"readingTime":8.362264150943396,"subtitle":"In part 2, we will have a comprehensive review of single shot object detectors including SSD and YOLO (YOLOv2 and YOLOv3). We will also…","usersBySocialRecommends":[],"noIndex":false,"recommends":327,"socialRecommends":[],"isBookmarked":false,"tags":[{"slug":"machine-learning","name":"Machine Learning","postCount":71701,"metadata":{"postCount":71701,"coverImage":{"id":"1*TDowJdT-tvhOmFzo79vCdQ.png","originalWidth":874,"originalHeight":422,"isFeatured":true}},"type":"Tag"},{"slug":"deep-learning","name":"Deep Learning","postCount":17656,"metadata":{"postCount":17656,"coverImage":{"id":"1*TDowJdT-tvhOmFzo79vCdQ.png","originalWidth":874,"originalHeight":422,"isFeatured":true}},"type":"Tag"},{"slug":"computer-vision","name":"Computer Vision","postCount":3507,"metadata":{"postCount":3507,"coverImage":{"id":"1*qYrPvxRwUeZAc7UmjH158Q.jpeg","originalWidth":4752,"originalHeight":3168,"isFeatured":true}},"type":"Tag"},{"slug":"object-detection","name":"Object Detection","postCount":569,"metadata":{"postCount":569,"coverImage":{"id":"0*E30eIZ5aCGjhCz9E.gif","originalWidth":600,"originalHeight":338,"isFeatured":true}},"type":"Tag"},{"slug":"artificial-intelligence","name":"Artificial Intelligence","postCount":84665,"metadata":{"postCount":84665,"coverImage":{"id":"1*gAn_BSffVBcwCIR6bDgK1g.jpeg"}},"type":"Tag"}],"socialRecommendsCount":0,"responsesCreatedCount":13,"links":{"entries":[{"url":"https://medium.com/@jonathan_hui/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c","alts":[{"type":2,"url":"medium://p/45b227b9106c"},{"type":3,"url":"medium://p/45b227b9106c"}],"httpStatus":200},{"url":"https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d","alts":[{"type":2,"url":"medium://p/3888677c5f4d"},{"type":3,"url":"medium://p/3888677c5f4d"}],"httpStatus":200},{"url":"https://medium.com/@jonathan_hui/design-choices-lessons-learned-and-trends-for-object-detections-4f48b59ec5ff","alts":[{"type":2,"url":"medium://p/4f48b59ec5ff"},{"type":3,"url":"medium://p/4f48b59ec5ff"}],"httpStatus":200},{"url":"https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088","alts":[{"type":2,"url":"medium://p/28b1b93e2088"},{"type":3,"url":"medium://p/28b1b93e2088"}],"httpStatus":200},{"url":"https://medium.com/@jonathan_hui/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06","alts":[{"type":2,"url":"medium://p/9bd8deac0e06"},{"type":3,"url":"medium://p/9bd8deac0e06"}],"httpStatus":200},{"url":"https://medium.com/@jonathan_hui/what-do-we-learn-from-region-based-object-detectors-faster-r-cnn-r-fcn-fpn-7e354377a7c9","alts":[{"type":2,"url":"medium://p/7e354377a7c9"},{"type":3,"url":"medium://p/7e354377a7c9"}],"httpStatus":200},{"url":"https://pjreddie.com/media/files/papers/YOLOv3.pdf","alts":[],"httpStatus":200},{"url":"https://arxiv.org/pdf/1612.03144.pdf","alts":[],"httpStatus":200},{"url":"https://arxiv.org/pdf/1708.02002.pdf","alts":[],"httpStatus":200},{"url":"https://arxiv.org/pdf/1512.02325.pdf","alts":[],"httpStatus":200},{"url":"https://arxiv.org/pdf/1612.08242.pdf","alts":[],"httpStatus":200}],"version":"0.3","generatedAt":1555349412890},"isLockedPreviewOnly":false,"metaDescription":"","totalClapCount":1974,"sectionCount":1,"readingList":0,"topics":[]},"coverless":true,"slug":"what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss","translationSourcePostId":"","translationSourceCreatorId":"","isApprovedTranslation":false,"inResponseToPostId":"","inResponseToRemovedAt":0,"isTitleSynthesized":true,"allowResponses":true,"importedUrl":"","importedPublishedAt":0,"visibility":0,"uniqueSlug":"what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d","previewContent":{"bodyModel":{"paragraphs":[{"name":"previewImage","type":4,"text":"","layout":10,"metadata":{"id":"0*OG_cQNTRdw0CYGyU.","originalWidth":3000,"originalHeight":1735,"isFeatured":true}},{"name":"4928","type":3,"text":"What do we learn from single shot object detectors (SSD, YOLOv3), FPN & Focal loss (RetinaNet)?","markups":[],"alignment":1}],"sections":[{"startIndex":0}]},"isFullContent":false,"subtitle":"In part 2, we will have a comprehensive review of single shot object detectors including SSD and YOLO (YOLOv2 and YOLOv3). We will also…"},"license":0,"inResponseToMediaResourceId":"","canonicalUrl":"https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d","approvedHomeCollectionId":"","newsletterId":"","webCanonicalUrl":"https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d","mediumUrl":"https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d","migrationId":"","notifyFollowers":true,"notifyTwitter":false,"notifyFacebook":false,"responseHiddenOnParentPostAt":0,"isSeries":false,"isSubscriptionLocked":false,"seriesLastAppendedAt":0,"audioVersionDurationSec":0,"sequenceId":"","isNsfw":false,"isEligibleForRevenue":false,"isBlockedFromHightower":false,"deletedAt":0,"lockedPostSource":0,"hightowerMinimumGuaranteeStartsAt":0,"hightowerMinimumGuaranteeEndsAt":0,"featureLockRequestAcceptedAt":0,"mongerRequestType":1,"layerCake":0,"socialTitle":"","socialDek":"","editorialPreviewTitle":"","editorialPreviewDek":"","curationEligibleAt":0,"type":"Post"},"mentionedUsers":[],"collaborators":[],"hideMeter":false,"collectionUserRelations":[],"mode":null,"references":{"User":{"bd51f1a63813":{"userId":"bd51f1a63813","name":"Jonathan Hui","username":"jonathan_hui","createdAt":1518417490123,"imageId":"1*c3Z3aOPBooxEX4tx4RkzLw.jpeg","backgroundImageId":"","bio":"Deep Learning","twitterScreenName":"","socialStats":{"userId":"bd51f1a63813","usersFollowedCount":16,"usersFollowedByCount":8172,"type":"SocialStats"},"social":{"userId":"lo_xxEwLfhil7sg","targetUserId":"bd51f1a63813","type":"Social"},"facebookAccountId":"","allowNotes":1,"mediumMemberAt":0,"isNsfw":false,"isWriterProgramEnrolled":true,"isQuarantined":false,"type":"User"}},"Social":{"bd51f1a63813":{"userId":"lo_xxEwLfhil7sg","targetUserId":"bd51f1a63813","type":"Social"}},"SocialStats":{"bd51f1a63813":{"userId":"bd51f1a63813","usersFollowedCount":16,"usersFollowedByCount":8172,"type":"SocialStats"}}}})
// ]]></script><script>window.PARSELY = window.PARSELY || { autotrack: false }</script><script id="parsely-cfg" src="//d1z2jf7jlzjs58.cloudfront.net/keys/medium.com/p.js"></script><script type="text/javascript">(function(b,r,a,n,c,h,_,s,d,k){if(!b[n]||!b[n]._q){for(;s<_.length;)c(h,_[s++]);d=r.createElement(a);d.async=1;d.src="https://cdn.branch.io/branch-latest.min.js";k=r.getElementsByTagName(a)[0];k.parentNode.insertBefore(d,k);b[n]=h}})(window,document,"script","branch",function(b,r){b[r]=function(){b._q.push([r,arguments])}},{_q:[],_v:1},"addListener applyCode autoAppIndex banner closeBanner closeJourney creditHistory credits data deepview deepviewCta first getCode init link logout redeem referrals removeListener sendSMS setBranchViewData setIdentity track validateCode trackCommerceEvent logEvent".split(" "), 0); branch.init('key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm', {'no_journeys': true, 'disable_exit_animation': true, 'disable_entry_animation': true, 'tracking_disabled':  false }, function(err, data) {});</script></body></html>